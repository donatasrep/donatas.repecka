{
  
    
        "post0": {
            "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
            "content": "Today, we are going to show you how to set up a literate programming environment, allowing you to use an IDE (VS Code) and an interactive computing environment (Jupyter), without leaving your browser, for free, in under 5 minutes. You’ll even see how VSCode and Jupyter work together automatically! But first, what is literate programming? And how did I go from skeptic to a zealot of literate programming? . Introduction . Literate programming is a programming paradigm introduced by Donald Knuth in which a computer program is given an explanation of its logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which compilable source code can be generated. According to Knuth, literate programming provides higher-quality programs by forcing programmers to explicitly state the thoughts behind the program. This process makes poorly thought-out design decisions more obvious. Knuth also claims that literate programming provides a first-rate documentation system, which is not an add-on, but is grown naturally in the process of exposition of one’s thoughts during a program’s creation. 1 . When I first learned about literate programming, I was quite skeptical. For the longest time, I had wrongly equated Jupyter notebooks with literate programming. Indeed, Jupyter is a brilliant interactive computing system, which was awarded the Association of Computing Machinery (ACM) Software System Award, and is loved by many developers. However, Jupyter falls short of the literate programming paradigm for the following reasons:2 . It can be difficult to compile source code from notebooks. | It can be difficult to diff and use version control with notebooks because they are not stored in plain text. | It is not clear how to automatically generate documentation from notebooks. | It is not clear how to properly run tests suites when writing code in notebooks. | . My skepticism quickly evaporated when I began using nbdev, a project that extends notebooks to complete the literate programming ideal. I spent a month, full time, using nbdev while contributing to the python library fastcore, and can report that Donald Knuth was definitely onto something. The process of writing prose and tests alongside code forced me to deeply understand why the code does what it does, and to think deeply about its design. Furthermore, the reduced cognitive load and speed of iteration of having documentation, code, and tests in one location boosted my productivity to levels I have never before experienced as a software developer. Furthermore, I found that developing this way bolstered collaboration such that code reviews not only happened faster but were more meaningful. In short, nbdev may be the most profound productivity tool I have ever used. . As a teaser, look how easy it is to instantiate this literate programming environment, which includes a notebook, a docs site and an IDE with all dependencies pre-installed! :point_down: . . Features of nbdev . As discussed in the docs, nbdev provides the following features: . Searchable, hyperlinked documentation, which can be automatically hosted on GitHub Pages for free. | Python modules, following best practices such as automatically defining __all__ with your exported functions, classes, and variables. | Pip and Conda installers. | Tests defined directly in notebooks which run in parallel. This testing system has been thoroughly tested with GitHub Actions. | Navigate and edit your code in a standard text editor or IDE, and export any changes automatically back into your notebooks. | . Since you are in a notebook, you can also add charts, text, links, images, videos, etc, that are included automatically in the documentation of your library, along with standardized documentation generated automatically from your code. This site is an example of docs generated automatically by nbdev. . GitHub Codespaces . Thanks to Conda and nbdev_template, setting up a development environment with nbdev is far easier than it used to be. However, we realized it could be even easier, thanks to a new GitHub product called Codespaces. Codespaces is a fully functional development environment in your browser, accessible directly from GitHub, that provides the following features: . A full VS Code IDE. | An environment that has files from the repository mounted into the environment, along with your GitHub credentials. | A development environment with dependencies pre-installed, backed by Docker. | The ability to serve additional applications on arbitrary ports. For nbdev, we serve a Jupyter notebook server as well as a Jekyll based documentation site. | A shared file system, which facilitates editing code in one browser tab and rendering the results in another. | … and more. | Codespaces enables developers to immediately participate in a project without wasting time on DevOps or complicated setup steps. Most importantly, CodeSpaces with nbdev allows developers to quickly get started with creating their own software with literate programming. . A demo of nbdev + Codespaces . This demo uses the project fastai/fastcore, which was built with nbdev, as an example. First, we can navigate to this repo and launch a Codespace: . . If you are launching a fresh Codespace, it may take several minutes to set up. Once the environment is ready, we can verify that all dependencies we want are installed (in this case fastcore and nbdev): . . Additionally, we can serve an arbitrary number of applications on user-specified ports, which we can open through VSCode as shown below: . . In this case, these applications are a notebook and docs site. Changes to a notebook are reflected immediately in the data docs. Furthermore, we can use the cli command nbdev_build_lib to sync our notebooks with python modules. This functionality is shown below: . . This is amazing! With a click of a button, I was able to: . Launch an IDE with all dependencies pre-installed. | Launch two additional applications: a Jupyter Notebook server on port 8080 and a docs site on port 4000. | Automatically update the docs and modules every time I make a change to a Jupyter notebook. | This is just the tip of the iceberg. There are additional utilities for writing and executing tests, diffing notebooks, special flags for hiding, showing, and collapsing cells in the generated docs, as well as git hooks for automation. This and more functionality is covered in the nbdev docs. . Give It A Try For Yourself . To try out nbdev yourself, take this tutorial, which will walk you through everything you need to know. The tutorial also shows you how to use a repository template with the configuration files necessary to enable Codespaces with nbdev. . You Can Write Blogs With Notebooks, Too! . This blog post was written in fastpages which is also built on nbdev! We recommend fastpages if you want an easy way to blog with Jupyter notebooks. . Additional Resources . The GitHub Codepaces site. | The official docs for Codespaces. | The nbdev docs. | The nbdev GitHub repo. | fastpages: The project used to write this blog. | The GitHub repo fastai/fastcore, which is what we used in this blog post as an example. | . Wikipedia article: Literate Programming &#8617; . | This is not a criticism of Jupyter. Jupyter doesn’t claim to be a full literate programming system. However, people can sometimes (unfairly) judge Jupyter according to this criteria. &#8617; . |",
            "url": "https://donatasrep.github.io/donatas.repecka/codespaces",
            "relUrl": "/codespaces",
            "date": " • Dec 10, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Hands-on Spectral Normalization",
            "content": "Spectral normalization is a widely used technique to stabilize and improve the training of Generative adversarial networks. In nutshell, this normalization technique allows the measurement of meaningful distance between real and generated examples using discriminator. This measure is then used to train both, the discriminator and the generator. . Introduction . In my previous post I have walked through changes proposed in WGAN paper skipping an import piece about Lipschitz constraint. In this post I will discuss the most effective (up to this day) technique to satisfy this constraint. . As we talked in previous post, work of the authors of WGAN paper proposed to use the distance between the outputs of the discriminator as a proxy for distances between distributions. To put it simply, we want to measure how far two distributions are from each other. In order to be able to use the discriminator output as a proxy, we need to bound discriminator to be Lipschitz constrained. . Spectral Normalization . Interestingly, the authors of WGAN paper turned to the AI scientific community for the best approach. And a year later the paper called Spectral Normalization for Generative Adversarial Networks came out with a proposed solution. In comparison to previous attempts, this solution was superior due to its efficiency. So let&#39;s dig into it. . According to the paper, the Lipschitz constraint of the layer can be satisfied by dividing the weights of the layer by the largest singular value of the same weights. A really nice explanation and proof can be found here. Mathematically it looks like this: . $$W=W/ sigma(W)$$ . where $ sigma(W)$ is the largest singular value of the weights $W$. . An important observation by the authors of the paper is that if we do it for all layers, we will have a network that satisfies Lipschitz constraint. . Sounds simple enough, let&#39;s try it out. . import numpy as np . Let&#39;s take a convolutional filter of kernel size 3x3, 256 input channels and 512 output channels. Note: we reshaped the kernel so that we could calculate singular values. . W = np.random.normal(size = [3,3, 256, 512]).reshape([-1,512]) W.shape . (2304, 512) . Let&#39;s use numpy library to find singular values. numpy.linalg.svd can do that for us. The second output of numpy.linalg.svd function returns all singular values, we just need a maximum of that. . %%timeit s = np.linalg.svd(W, full_matrices=True)[1].max() s . 1 loop, best of 3: 1.02 s per loop . That is quite simple, however, it takes 1 second to calculate those values. Let&#39;s assume that we have 30 layers in the network, that would mean extra 30 seconds for each training step. Well, that is not what we could call efficient. . from scipy.sparse.linalg import svds . There is an alternative. scipy.sparse.linalg.svds function returns only the number k of the largest singular values. . %%timeit s = svds(W, k=1)[1] s . 10 loops, best of 3: 72 ms per loop . This is much faster: ~74ms, however for 30 layers we will slow down the training process by 2 seconds per step. Still not ideal. . Luckly, The authors of the paper proposed an alternative solution to get the largest singular value. This technique is known as power iteration. This looks like this: $$ v = W^ intercal u / ||W^ intercal u||_2$$ . $$ u = W v / ||W v||_2$$ . $$ W = W / u^ intercal W v$$ . Let&#39;s see if we can implement it. . $u$ needs to be sampled from an isotropic distribution at the beginning and it&#39;s dimensions should match the dimensions of the number of output channels. . u = np.random.normal(scale=0.2, size=[512]) u.shape . (512,) . First row of the equation . Note: we swapped u and W just for the sake of simplicity when implementing in numpy . v = u@W.T/np.linalg.norm(u@W.T, 2) v.shape . (2304,) . Second row of the equation . u = v@W / np.linalg.norm(v@W, 2) u.shape . (512,) . Third row of the equation . sigma = v@W@u.T sigma . 53.890708570486424 . Let&#39;s put everything together . def get_largets_singular_value(u): v = u@W.T/np.linalg.norm(u@W.T, 2) u = v@W / np.linalg.norm(v@W, 2) sigma = v@W@u.T return sigma, u . %%timeit sigma, _ = get_largets_singular_value(u.copy()) . 100 loops, best of 3: 2.06 ms per loop . print(&quot;Answer from numpy:&quot;, svds(W, k=1)[1].squeeze()) print(&quot;Power iteration outcome&quot;, sigma) . Answer from numpy: 70.21844081626173 Power iteration outcome 69.4230148155534 . Well, it is fast, ~40x faster, but it is not accurate. Well, that is expected because power iteration only approximates the largest singular value. The more iterations you perform, the more accurate approximation you would get. . Let’s see if that is true. . u_copy = u.copy() actual = svds(W, k=1)[1].squeeze() for i in range(20): sigma,u_copy = get_largets_singular_value(u_copy) print(&quot;Power iteration estimate: {:.2f} (actual: {:.2f})&quot;.format(sigma, actual)) . Power iteration estimate: 60.51 (actual: 70.22) Power iteration estimate: 63.85 (actual: 70.22) Power iteration estimate: 65.64 (actual: 70.22) Power iteration estimate: 66.69 (actual: 70.22) Power iteration estimate: 67.36 (actual: 70.22) Power iteration estimate: 67.82 (actual: 70.22) Power iteration estimate: 68.15 (actual: 70.22) Power iteration estimate: 68.40 (actual: 70.22) Power iteration estimate: 68.59 (actual: 70.22) Power iteration estimate: 68.74 (actual: 70.22) Power iteration estimate: 68.86 (actual: 70.22) Power iteration estimate: 68.97 (actual: 70.22) Power iteration estimate: 69.06 (actual: 70.22) Power iteration estimate: 69.13 (actual: 70.22) Power iteration estimate: 69.19 (actual: 70.22) Power iteration estimate: 69.25 (actual: 70.22) Power iteration estimate: 69.30 (actual: 70.22) Power iteration estimate: 69.35 (actual: 70.22) Power iteration estimate: 69.39 (actual: 70.22) Power iteration estimate: 69.42 (actual: 70.22) . This is indeed the case. However, doing power iteration 20 times gets us close to scipy performance. Fortunately, Spectral Normalization paper showed that you can do one or a small number of iterations per training step to get a good estimate throughout the entire training (doing incremental work every step). Hence, even using naive numpy implementation we can get to only extra 60ms (1.7ms*30) for each training iteration (assuming 30 layer network). . That does not sound too bad. . Note: keep in mind that these performance numbers are relative, they will depend on hardware. . We can try to port this to tensorflow and measure its performance. . import tensorflow as tf tf.config.list_physical_devices(&#39;GPU&#39;) . [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . @tf.function def get_largets_singular_value_tf(u, w): _v = tf.matmul(u, w, transpose_b=True) _v = tf.math.l2_normalize(_v) _u_m = tf.matmul(_v, w) _u = tf.math.l2_normalize(_u_m) sigma = tf.matmul(_u_m, _u, transpose_b=True) return sigma, _u . u_tf = tf.Variable(tf.random.normal(stddev=0.2, shape=[1,512]), trainable=False) w_tf = tf.Variable(tf.random.normal(shape = [3*3*256, 512]), trainable=False) . %%timeit sigma_tf, _ = get_largets_singular_value_tf(u_tf, w_tf) . The slowest run took 65.11 times longer than the fastest. This could mean that an intermediate result is being cached. 1000 loops, best of 3: 509 µs per loop . for i in range(20): sigma_tf, u_tf = get_largets_singular_value_tf(u_tf, w_tf) print(sigma_tf) . tf.Tensor([[69.43079]], shape=(1, 1), dtype=float32) . That is even better. 509µs with tensorflow implementation and executed on GPU (which usually is the case when you are training GANs). So if we assume 30 layers of similar size and one power iteration per training step, we only need ~15ms extra for the step. That is the reason why this technique is widely and successfully adopted. . In practice . Since we have all the pieces, we can create a layer wrapper to wrap any layer and perform power iteration on each feed forward pass. I have extended the code of here I have incorporated suggestions from the comments: . Turned off power iterations during the inherence. This makes sure that weights are not being changed doing inherence. | Used assigned operation instead of =. This speeds up the algorithm quite significantly (roughly by 35%). | . Additionally I have also added a hack to support mixed precision as well as the logic to support embedding layers. The final solution looks like this: . class SpectralNormalizationV2(tf.keras.layers.Wrapper): &quot;&quot;&quot; Attributes: layer: tensorflow keras layers (with kernel or embedding attribute) &quot;&quot;&quot; def __init__(self, layer, eps=1e-12, **kwargs): super(SpectralNormalizationV2, self).__init__(layer, name=layer.name + &quot;_sn&quot;, **kwargs) self.eps = eps self.is_embedding = isinstance(self.layer, tf.keras.layers.Embedding) def get_kernel_variable(self, attr=&#39;kernel&#39;): if not hasattr(self.layer, attr): raise ValueError(&#39;`SpectralNormalization` must wrap a layer that contains a `{}` for weights&#39;.format(attr)) return getattr(self.layer, attr) def build(self, input_shape=None): if not self.built: super(SpectralNormalizationV2, self).build(input_shape) if self.is_embedding: self.w = self.get_kernel_variable(&quot;embeddings&quot;) else: self.w = self.get_kernel_variable() self.autocast = hasattr(self.w, &quot;_variable&quot;) self.last_dim = self.w.shape[-1] self.u = self.add_weight(shape=[1, self.last_dim], initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02), name=&#39;sn_u&#39;, trainable=False, experimental_autocast=False) @tf.function def call(self, inputs, training=True): # Recompute weights for each training forward pass if training: self._compute_weights() output = self.layer(inputs, training=training) return output def _compute_weights(self): &quot;&quot;&quot;Generate normalized weights. This method will update the value of self.layer.kernel with the normalized value, so that the layer is ready for call(). &quot;&quot;&quot; if self.autocast: w = self.w._variable else: w =self.w w_reshaped = tf.reshape(w, [-1, self.last_dim]) _v = tf.matmul(self.u, w_reshaped, transpose_b=True) _v = tf.math.l2_normalize(_v, epsilon=self.eps) _u_m = tf.matmul(_v, w_reshaped) _u = tf.math.l2_normalize(_u_m, epsilon=self.eps) sigma = tf.matmul(_u_m, _u, transpose_b=True) self.u.assign(_u) self.w.assign(w / sigma) def compute_output_shape(self, input_shape): return self.layer.compute_output_shape(input_shape) . And the way you use is: . dense_sn = SpectralNormalizationV2(tf.keras.layers.Dense(units=100)) conv_sn = SpectralNormalizationV2(tf.keras.layers.Conv2D(filters=256, kernel_size=3)) emb_sn = SpectralNormalizationV2(tf.keras.layers.Embedding(input_dim=20, output_dim=100)) . I have also checked the performance in comparison to Pytorch implementation . Setup: . Batch size: 64 | Kernel of size :[16, 16, 256, 512] | Steps: 1000 | One power iteration per step | Hardware: NVIDIA V100 | . Results are: . Elapse time of Tensorflow SpectralNormalizationV2: 1.1081128120422363s | Elapse time of Pytorch official SpectralNorm implementation: 1.0783729553222656s | . Pytorch seems to be a little bit faster. I am not entirely sure whether it has to do with implementations of spectral normalization or just to the differences between Pytorch and Tensorflow. Regardless, those extra 30ms every 1000 steps should not be a game changer. . Summary . Spectral normalization is quite widely used in various implementations of GANs. One of the most famous applications is Biggan. All images below are generated by BigGAN architecture that uses Spectral normalization. This should highlight the importance of this advancement. . Throughout this post we focused on the implementation of power iteration proposed in Spectral Normalization for Generative Adversarial Networks paper. We have compared various methods to calculate the largest singular value and observed that the proposed power iteration method is the fastest and after multiple iterations it achieves reasonable accuracy. . .",
            "url": "https://donatasrep.github.io/donatas.repecka/2020/04/08/spectral-norm.html",
            "relUrl": "/2020/04/08/spectral-norm.html",
            "date": " • Apr 8, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Why we use WGAN?",
            "content": "Introduction . The original version of GAN was released in 2014. It was quite revolutionary and sparked the new line of research that is still very active. What is so revolutionary about GANs? The authors of the paper proposed to train two neural networks (discriminator and generator) stacked against each other in order to overcome the problem of having to create a loss function for the generator. . Now it seems rather obvious, however, at the time it was a wild idea. And even if it looked mathematically feasible, there was no indication that it could work in practise. Nevertheless, Ian Goodfellow and his colleagues demonstrated that such architecture can actually work even with resources at the time. See an examples from the same paper below (Note: images in yellow are the closest exmamples from training dataset) . Source: https://arxiv.org/pdf/1406.2661.pdf . In nutshell, generative adversarial networks consist of two networks: generator which generates data points and discriminator that evaluates whether input is coming from training dataset or is generated by generator. It is simple enough, however, it is quite important how the task of discriminator is formulated. That will be the main theme of this post. . Original discriminator objective . In the original version of GAN measure of real and generated examples was formulated as binary classification: is it real or is it generated. Let&#39;s look at the simple case, where we have true distribution (in other words real examples) that consists of 0&#39;s. A perfect discriminator will return 1 when input to the discriminator is 0 and output of the discriminator would be 0 otherwise. Despite the fact that discriminator is doing a great job at discriminating between real and generated, however, the generator does not receive meaningful direction for improving. For example, generator output of 0.1 and 1 will be scored the same, even though 0.1 is much closer to real distribution. Let&#39;s see this in action. . import tensorflow as tf import numpy as np from matplotlib import pyplot as plt np.set_printoptions(suppress=False, precision=5) . Discriminator . def d_loss(d_score_z, d_score_r): batch_size = d_score_z.shape[0] d_loss_z =tf.keras.losses.binary_crossentropy(tf.zeros(shape = [batch_size,1], dtype=tf.float32), d_score_z) d_loss_r =tf.keras.losses.binary_crossentropy(tf.ones(shape = [batch_size,1], dtype=tf.float32), d_score_r) return tf.reduce_mean(d_loss_z + d_loss_r) def define_discriminator(act=&quot;sigmoid&quot;): d_optimizer = tf.keras.optimizers.Adam(learning_rate=0.05) # Discriminator discriminator = tf.keras.models.Sequential() discriminator.add(tf.keras.layers.Dense(32, activation=&quot;relu&quot;)) discriminator.add(tf.keras.layers.Dense(32, activation=&quot;relu&quot;)) discriminator.add(tf.keras.layers.Dense(1, activation=act)) return discriminator, d_optimizer . def train_discriminator(discriminator, d_optimizer, d_loss, batch_size = 64): real_data = tf.zeros([batch_size, 1]) # Train discriminator till perfection for i in range(1000): with tf.GradientTape() as tape: z = tf.random.normal(shape=[batch_size, 1]) d_score_z, d_score_r = discriminator(z), discriminator(real_data) total_loss = d_loss(d_score_z, d_score_r) if i % 100 == 0: print(&quot;Total loss:&quot;, total_loss.numpy(), &quot;Scores for real:&quot;, d_score_r.numpy().mean(), &quot;Scores for generated:&quot;, d_score_z.numpy().mean()) grads = tape.gradient(total_loss, discriminator.trainable_variables) d_optimizer.apply_gradients(zip(grads, discriminator.trainable_variables)) . Disclaimer: we are creating an artificial situation just to demonstrate the underlying issue with using binary classification as a loss for discriminator. In reality, we would not train discriminators in this fashion. . discriminator, d_optimizer = define_discriminator() train_discriminator(discriminator, d_optimizer, d_loss) . Total loss: 1.2753901 Scores for real: 0.5 Scores for generated: 0.43882957 Total loss: 0.07295962 Scores for real: 0.9781779 Scores for generated: 0.015023634 Total loss: 0.014039941 Scores for real: 0.986065 Scores for generated: 7.0891906e-06 Total loss: 0.021772312 Scores for real: 0.9784628 Scores for generated: 5.510665e-11 Total loss: 0.075927064 Scores for real: 0.9949504 Scores for generated: 0.015457453 Total loss: 0.11446469 Scores for real: 0.9977589 Scores for generated: 0.038589276 Total loss: 0.00037504316 Scores for real: 0.99962485 Scores for generated: 4.787702e-14 Total loss: 0.07557466 Scores for real: 0.9988571 Scores for generated: 0.017265305 Total loss: 0.0038048518 Scores for real: 0.9962043 Scores for generated: 2.0079315e-06 Total loss: 0.0021338563 Scores for real: 0.9978684 Scores for generated: 6.928249e-14 . So we have got a nearly perfect discriminator that gives 1 for real distribution and 0 for anything else. Let&#39;s train the generator using such discriminator. . Generator . Here we will try to train the generator using perfect discriminator. Again, you would not do that in practise. . def g_loss(x, discriminator=discriminator): return tf.reduce_mean(1.0-discriminator(x)) def train_generator(g_loss, batch_size=64): g_optimizer = tf.keras.optimizers.Adam(learning_rate=0.05) generator = tf.keras.layers.Dense(1, activation=&#39;tanh&#39;) for i in range(1000): with tf.GradientTape() as tape: z = tf.random.normal(shape=[batch_size, 1]) gen = generator(z) loss = g_loss(gen) if i % 100 == 0: print(&quot;Loss:&quot;, loss.numpy(), &quot;Generator output: &quot;,tf.reduce_mean(tf.abs(gen)).numpy()) grads = tape.gradient(loss, generator.trainable_variables) g_optimizer.apply_gradients(zip(grads, generator.trainable_variables)) t = plt.scatter(np.zeros(z.shape), z.numpy(), label=&quot;True&quot;) g = plt.scatter(gen.numpy(), z.numpy(), alpha=0.5) plt.xlim(-1, 1) plt.legend((t,g),(&quot;True distribution&quot;, &quot;Generated distribution&quot;)) . train_generator(g_loss) . Loss: 0.98446023 Generator output: 0.5757177 Loss: 1.0 Generator output: 0.62681496 Loss: 0.99996835 Generator output: 0.68048704 Loss: 1.0 Generator output: 0.6950605 Loss: 1.0 Generator output: 0.7098987 Loss: 1.0 Generator output: 0.7288726 Loss: 1.0 Generator output: 0.9237368 Loss: 1.0 Generator output: 0.9254451 Loss: 1.0 Generator output: 0.922577 Loss: 1.0 Generator output: 0.923764 . It does not converge, the generator does not learn to produce 0&#39;s and loss does not decrease. We could find a way to train so that it converges, however, ideally we would want something that would be able to converge in this situation as well. . This issue was described in famous Wasserstein GAN paper. In this paper authors proposed to change binary classification to something that measures distance between generated outputs and real examples. . Let&#39;s try it out with our simplified example. . WGAN . A very naive approach would be to measure Euclidean distance between real and generated distributions. Let’s see if that helps. . def euclidean_distance(x): return tf.reduce_mean(tf.norm(x, &#39;euclidean&#39;)) train_generator(euclidean_distance) . Loss: 1.8566905 Generator output: 0.19034971 Loss: 0.06090928 Generator output: 0.005873415 Loss: 0.05984832 Generator output: 0.0073457574 Loss: 0.025419286 Generator output: 0.0026790632 Loss: 0.026378132 Generator output: 0.0026014717 Loss: 0.031021483 Generator output: 0.0037481724 Loss: 0.05135456 Generator output: 0.0057523246 Loss: 0.14441356 Generator output: 0.014102172 Loss: 0.059984554 Generator output: 0.0058901818 Loss: 0.053461727 Generator output: 0.0066189137 . It works in this example, we can see that loss is decreasing and generator output approaches 0. However, this works in this specific scenario, we can use euclidean distance when we measure the distance between 1D points, but how about images? Hence, for a general solution we will need something that works universally. And that is what Wasserstein GAN paper is about. . For the sake of this post, we will skip the details of how and why WGAN works (there are a few brilliant posts about that, please checkout out links in &quot;Further reading&quot; section below), but to put it simply, we can use the distance between the outputs of the discriminator as proxy for distances between distributions. (For this to work, we have to enforce a Lipschitz constraint which we will skip in this post, but will talk about how to enforce it in the next one.) . Let&#39;s try it out. . We measure the distance between the outputs of discriminator by simply subtracting one from another. . def d_loss_wgan(d_score_z, d_score_r): return tf.reduce_mean(d_score_z - d_score_r) . discriminator_wgan, d_optimizer = define_discriminator(act=None) train_discriminator(discriminator_wgan, d_optimizer, d_loss_wgan) . Total loss: -0.20631 Scores for real: 0.0 Scores for generated: -0.20631 Total loss: -92241.375 Scores for real: -128.61824 Scores for generated: -92370.0 Total loss: -1122561.8 Scores for real: -766.5613 Scores for generated: -1123328.2 Total loss: -4381084.5 Scores for real: -1648.9462 Scores for generated: -4382733.5 Total loss: -11101716.0 Scores for real: -2930.0134 Scores for generated: -11104646.0 Total loss: -20111968.0 Scores for real: -2881.5947 Scores for generated: -20114848.0 Total loss: -37373372.0 Scores for real: -2529.8655 Scores for generated: -37375904.0 Total loss: -62526256.0 Scores for real: -10314.391 Scores for generated: -62536570.0 Total loss: -69934790.0 Scores for real: -9289.29 Scores for generated: -69944080.0 Total loss: -135784350.0 Scores for real: -2764.6501 Scores for generated: -135787120.0 . Looks like it is working, loss is decreasing (although it is quite unusual to see negative loss). . So now we have discriminator which gives high scores for real examples. So the objective for generator would be to get as high scores as possible, hence we need just -discriminator(generated) . def g_loss_wgan(x): return tf.reduce_mean(-discriminator_wgan(x)) train_generator(g_loss_wgan) . Loss: 86786170.0 Generator output: 0.3962338 Loss: 2060925.2 Generator output: 0.009125391 Loss: 2406305.5 Generator output: 0.011163255 Loss: 816975.6 Generator output: 0.0036342242 Loss: 1002175.4 Generator output: 0.004520608 Loss: 1894261.4 Generator output: 0.008713374 Loss: 2398592.2 Generator output: 0.011115221 Loss: 1497964.8 Generator output: 0.007058435 Loss: 2221005.2 Generator output: 0.010142955 Loss: 2540864.5 Generator output: 0.011403117 . Code above actually is in line with WGAN (except, as I mentioned before, we are missing a method to enforce Lipschitz constraint). Three main changes are: . we removed sigmoid from the last layer of discriminator because we are not doing classification anymore. | we change the loss of discriminator to be the distance between real and generated outputs of discriminator (subtraction will do just fine for measuring distance between 1D points) | we change the generator loss to be just opposite to discriminator loss for generated input. | . We can see that this algorithm converges and the generated distribution approaches 0. This is all nice and good, however, as you can see, discriminator loss is actually a negative number, hence practitioners more often use some sort of not saturating loss. For example, hinge loss which is defined as follows: . def d_loss_hinge(d_score_z, d_score_r): d_loss_real = tf.nn.relu(1.0 - d_score_r) # Note: this changed d_loss_fake = tf.nn.relu(1.0 + d_score_z) # Note: this changed return tf.reduce_mean(d_loss_real) + tf.reduce_mean(d_loss_fake) . discriminator_hinge, d_optimizer = define_discriminator(act=None) train_discriminator(discriminator_hinge, d_optimizer, d_loss_hinge) . Total loss: 2.2130427 Scores for real: 0.0 Scores for generated: 0.21304269 Total loss: 0.046959918 Scores for real: 1.5518551 Scores for generated: -64.95206 Total loss: 0.0864621 Scores for real: 1.9289998 Scores for generated: -83.80411 Total loss: 0.035902765 Scores for real: 2.3413584 Scores for generated: -120.67704 Total loss: 0.06848332 Scores for real: 1.2159318 Scores for generated: -131.21864 Total loss: 0.07119481 Scores for real: 1.7761744 Scores for generated: -161.72647 Total loss: 0.041249227 Scores for real: 1.805813 Scores for generated: -213.31772 Total loss: 0.0 Scores for real: 1.5168437 Scores for generated: -301.31628 Total loss: 0.11920573 Scores for real: 1.8994367 Scores for generated: -270.10626 Total loss: 0.0 Scores for real: 1.4863261 Scores for generated: -312.72998 . The same non saturaing principle can be applied to the generator . def g_loss_hinge(x): return tf.reduce_mean(tf.nn.relu(1.0-discriminator_hinge(x))) . train_generator(g_loss_hinge) . Loss: 251.17415 Generator output: 0.50602597 Loss: 0.0 Generator output: 0.009517897 Loss: 0.0 Generator output: 0.008805303 Loss: 0.0 Generator output: 0.008805184 Loss: 0.0 Generator output: 0.008802602 Loss: 0.0 Generator output: 0.008800652 Loss: 0.0 Generator output: 0.008805037 Loss: 0.0 Generator output: 0.00880926 Loss: 0.0 Generator output: 0.008802065 Loss: 0.0 Generator output: 0.008806262 . That looks good, we are nearly done, but as I mentioned, we skipped &quot;Lipschitz constraint&quot; piece from Wasserstein GAN paper. We will focus on this in the next post. . Summary . I hope now you intuitively understand why we moved from the original GAN to the idea of having the discriminator that measures distance rather than classifying the input. Although the code changes are quite simple, they did improve the stability of the training significantly. And this advancement of GANs played an important role to allow us to generate pictures like that today. . . Further Reading . Read-through: Wasserstein GAN | From GAN to WGAN | GAN — Wasserstein GAN &amp; WGAN-GP | .",
            "url": "https://donatasrep.github.io/donatas.repecka/2020/04/05/gan-wgan.html",
            "relUrl": "/2020/04/05/gan-wgan.html",
            "date": " • Apr 5, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "GitHub Actions: Providing Data Scientists With New Superpowers",
            "content": "What Superpowers? . Hi, I’m Hamel Husain. I’m a machine learning engineer at GitHub. Recently, GitHub released a new product called GitHub Actions, which has mostly flown under the radar in the machine learning and data science community as just another continuous integration tool. . Recently, I’ve been able to use GitHub Actions to build some very unique tools for Data Scientists, which I want to share with you today. Most importantly, I hope to get you excited about GitHub Actions, and the promise it has for giving you new superpowers as a Data Scientist. Here are two projects I recently built with Actions that show off its potential: . fastpages . fastpages is an automated, open-source blogging platform with enhanced support for Jupyter notebooks. You save your notebooks, markdown, or Word docs into a directory on GitHub, and they automatically become blog posts. Read the announcement below: . We&#39;re launching `fastpages`, a platform which allows you to host a blog for free, with no ads. You can blog with @ProjectJupyter notebooks, @office Word, directly from @github&#39;s markdown editor, etc.Nothing to install, &amp; setup is automated!https://t.co/dNSA0oQUrN . &mdash; Jeremy Howard (@jeremyphoward) February 24, 2020 Machine Learning Ops . Wouldn’t it be cool if you could invoke a chatbot natively on GitHub to test your machine learning models on the infrastructure of your choice (GPUs), log all the results, and give you a rich report back in a pull request so that everyone could see the results? You can with GitHub Actions! . Consider the below annotated screenshot of this Pull Request: . . A more in-depth explanation about the above project can be viewed in this video: . Using GitHub Actions for machine learning workflows is starting to catch on. Julien Chaumond, CTO of Hugging Face, says: . GitHub Actions are great because they let us do CI on GPUs (as most of our users use the library on GPUs not on CPUs), on our own infra! 1 . Additionally, you can host a GitHub Action for other people so others can use parts of your workflow without having to re-create your steps. I provide examples of this below. . A Gentle Introduction To GitHub Actions . What Are GitHub Actions? . GitHub Actions allow you to run arbitrary code in response to events. Events are activities that happen on GitHub such as: . Opening a pull request | Making an issue comment | Labeling an issue | Creating a new branch | … and many more | . When an event is created, the GitHub Actions context is hydrated with a payload containing metadata for that event. Below is an example of a payload that is received when an issue is created: . { &quot;action&quot;: &quot;created&quot;, &quot;issue&quot;: { &quot;id&quot;: 444500041, &quot;number&quot;: 1, &quot;title&quot;: &quot;Spelling error in the README file&quot;, &quot;user&quot;: { &quot;login&quot;: &quot;Codertocat&quot;, &quot;type&quot;: &quot;User&quot;, }, &quot;labels&quot;: [ { &quot;id&quot;: 1362934389, &quot;node_id&quot;: &quot;MDU6TGFiZWwxMzYyOTM0Mzg5&quot;, &quot;name&quot;: &quot;bug&quot;, } ], &quot;body&quot;: &quot;It looks like you accidently spelled &#39;commit&#39; with two &#39;t&#39;s.&quot; } . This functionality allows you to respond to various events on GitHub in an automated way. In addition to this payload, GitHub Actions also provide a plethora of variables and environment variables that afford easy to access metadata such as the username and the owner of the repo. Additionally, other people can package useful functionality into an Action that other people can inherit. For example, consider the below Action that helps you publish python packages to PyPi: . The Usage section describes how this Action can be used: . - name: Publish a Python distribution to PyPI uses: pypa/gh-action-pypi-publish@master with: user: __token__ password: ${{ secrets.pypi_password }} . This Action expects two inputs: user and a password. You will notice that the password is referencing a variable called secrets, which is a variable that contains an encrypted secret that you can upload to your GitHub repository. There are thousands of Actions (that are free) for a wide variety of tasks that can be discovered on the GitHub Marketplace. The ability to inherit ready-made Actions in your workflow allows you to accomplish complex tasks without implementing all of the logic yourself. Some useful Actions for those getting started are: . actions/checkout: Allows you to quickly clone the contents of your repository into your environment, which you often want to do. This does a number of other things such as automatically mount your repository’s files into downstream Docker containers. | mxschmitt/action-tmate: Proivdes a way to debug Actions interactively. This uses port forwarding to give you a terminal in the browser that is connected to your Actions runner. Be careful not to expose sensitive information if you use this. | actions/github-script: Gives you a pre-authenticated ocotokit.js client that allows you to interact with the GitHub API to accomplish almost any task on GitHub automatically. Only these endpoints are supported (for example, the secrets endpoint is not in that list). | . In addition to the aforementioned Actions, it is helpful to go peruse the official GitHub Actions docs before diving in. . Example: A fastpages Action Workflow . The best to way familiarize yourself with Actions is by studying examples. Let’s take a look at the Action workflow that automates the build of fastpages (the platform used to write this blog post). . Part 1: Define Workflow Triggers . First, we define triggers in ci.yaml. Like all Actions workflows, this is a YAML file located in the .github/workflows directory of the GitHub repo. . The top of this YAML file looks like this: . name: CI on: push: branches: - master pull_request: . This means that this workflow is triggered on either a push or pull request event. Furthermore, push events are filtered such that only pushes to the master branch will trigger the workflow, whereas all pull requests will trigger this workflow. It is important to note that pull requests opened from forks will have read-only access to the base repository and cannot access any secrets for security reasons. The reason for defining the workflow in this way is we wanted to trigger the same workflow to test pull requests as well as build and deploy the website when a PR is merged into master. This will be clarified as we step through the rest of the YAML file. . Part 2: Define Jobs . Next, we define jobs (there is only one in this workflow). Per the docs: . A workflow run is made up of one or more jobs. Jobs run in parallel by default. . jobs: build-site: if: ( github.event.commits[0].message != &#39;Initial commit&#39; ) || github.run_number &gt; 1 runs-on: ubuntu-latest steps: . The keyword build-site is the name of your job and you can name it whatever you want. In this case, we have a conditional if statement that dictates if this job should be run or not. We are trying to ensure that this workflow does not run when the first commit to a repo is made with the message ‘Initial commit’. The first variable in the if statement, github.event, contains a json payload of the event that triggered this workflow. When developing workflows, it is helpful to print this variable in order to inspect its structure, which you can accomplish with the following YAML: . - name: see payload run: | echo &quot;PAYLOAD: n${PAYLOAD} n&quot; env: PAYLOAD: ${{ toJSON(github.event) }} . Note: the above step is only for debugging and is not currently in the workflow. . toJson is a handy function that returns a pretty-printed JSON representation of the variable. The output is printed directly in the logs contained in the Actions tab of your repo. In this example, printing the payload for a push event will look like this (truncated for brevity): . { &quot;ref&quot;: &quot;refs/tags/simple-tag&quot;, &quot;before&quot;: &quot;6113728f27ae8c7b1a77c8d03f9ed6e0adf246&quot;, &quot;created&quot;: false, &quot;deleted&quot;: true, &quot;forced&quot;: false, &quot;base_ref&quot;: null, &quot;commits&quot;: [ { &quot;message&quot;: &quot;updated README.md&quot;, &quot;author&quot;: &quot;hamelsmu&quot; }, ], &quot;head_commit&quot;: null, } . Therefore, the variable github.event.commits[0].message will retrieve the first commit message in the array of commits. Since we are looking for situations where there is only one commit, this logic suffices. The second variable in the if statement, github.run_number is a special variable in Actions which: . [is a] unique number for each run of a particular workflow in a repository. This number begins at 1 for the workflow’s first run, and increments with each new run. This number does not change if you re-run the workflow run. . Therefore, the if statement introduced above: . if: ( github.event.commits[0].message != &#39;Initial commit&#39; ) || github.run_number &gt; 1 . Allows the workflow to run when the commit message is “Initial commit” as long as it is not the first commit. ( || is a logical or operator). . Finally, the line runs-on: ubuntu-latest specifies the host operating system that your workflows will run in. . Part 3: Define Steps . Per the docs: . A job contains a sequence of tasks called steps. Steps can run commands, run setup tasks, or run an Action in your repository, a public repository, or an Action published in a Docker registry. Not all steps run Actions, but all Actions run as a step. Each step runs in its own process in the runner environment and has access to the workspace and filesystem. Because steps run in their own process, changes to environment variables are not preserved between steps. GitHub provides built-in steps to set up and complete a job. . Below are the first two steps in our workflow: . - name: Copy Repository Contents uses: actions/checkout@master with: persist-credentials: false - name: convert notebooks and word docs to posts uses: ./_action_files . The first step creates a copy of your repository in the Actions file system, with the help of the utility action/checkout. This utility only fetches the last commit by default and saves files into a directory (whose path is stored in the environment variable GITHUB_WORKSPACE that is accessible by subsequent steps in your job. The second step runs the fastai/fastpages Action, which converts notebooks and word documents to blog posts automatically. In this case, the syntax: . uses: ./_action_files . is a special case where the pre-made GitHub Action we want to run happens to be defined in the same repo that runs this workflow. This syntax allows us to test changes to this pre-made Action when evaluating PRs by referencing the directory in the current repository that defines that pre-made Action. Note: Building pre-made Actions is beyond the scope of this tutorial. . The next three steps in our workflow are defined below: . - name: setup directories for Jekyll build run: | rm -rf _site sudo chmod -R 777 . - name: Jekyll build uses: docker://fastai/fastpages-jekyll with: args: bash -c &quot;gem install bundler &amp;&amp; jekyll build -V&quot; env: JEKYLL_ENV: &#39;production&#39; - name: copy CNAME file into _site if CNAME exists run: | sudo chmod -R 777 _site/ cp CNAME _site/ 2&gt;/dev/null || : . The step named setup directories for Jekyll build executes shell commands that remove the _site folder in order to get rid of stale files related to the page we want to build, as well as grant permissions to all the files in our repo to subsequent steps. . The step named Jekyll build executes a docker container hosted by the Jekyll community on Dockerhub called jekyll/jekyll. For those not familiar with Docker, see this tutorial. The name of this container is called fastai/fastpages-jekyll because I’m adding some additional dependencies to jekyll/jekyll and hosting those on my DockerHub account for faster build times2. The args parameter allows you to execute arbitrary commands with the Docker container by overriding the CMD instruction in the Dockerfile. We use this Docker container hosted on Dockerhub so we don’t have to deal with installing and configuring all of the complicated dependencies for Jekyll. The files from our repo are already available in the Actions runtime due to the first step in this workflow, and are mounted into this Docker container automatically for us. In this case, we are running the command jekyll build, which builds our website and places relevant assets them into the _site folder. For more information about Jekyll, read the official docs. Finally, the env parameter allows me to pass an environment variable into the Docker container. . The final command above copies a CNAME file into the _site folder, which we need for the custom domain https://fastpages.fast.ai. Setting up custom domains are outside the scope of this article. . The final step in our workflow is defined below: . - name: Deploy if: github.event_name == &#39;push&#39; uses: peaceiris/actions-gh-pages@v3 with: deploy_key: ${{ secrets.SSH_DEPLOY_KEY }} publish_dir: ./_site . The statement . if: github.event_name == &#39;push&#39; . uses the variable github.event_name to ensure this step only runs when a push event ( in this case only pushes to the master branch trigger this workflow) occur. . This step deploys the fastpages website by copying the contents of the _site folder to the root of the gh-pages branch, which GitHub Pages uses for hosting. This step uses the peaceiris/actions-gh-pages Action, pinned at version 3. Their README describes various options and inputs for this Action. . Conclusion . We hope that this has shed some light on how we use GitHub Actions to automate fastpages. While we only covered one workflow above, we hope this provides enough intuition to understand the other workflows in fastpages. We have only scratched the surface of GitHub Actions in this blog post, but we provide other materials below for those who want to dive in deeper. We have not covered how to host an Action for other people, but you can start with these docs to learn more. . Still confused about how GitHub Actions could be used for Data Science? Here are some ideas of things you can build: . Jupyter Widgets that trigger GitHub Actions to perform various tasks on GitHub via the repository dispatch event | Integration with Pachyderm for data versioning. | Integration with your favorite cloud machine learning services, such Sagemaker, Azure ML or GCP’s AI Platform. | . Related Materials . GitHub Actions official documentation | Hello world Docker Action: A template to demonstrate how to build a Docker Action for other people to use. | Awesome Actions: A curated list of interesting GitHub Actions by topic. | A tutorial on Docker for Data Scientists. | . Getting In Touch . Please feel free to get in touch with us on Twitter: . Hamel Husain @HamelHusain | Jeremy Howard @jeremyphoward | . . Footnotes . You can see some of Hugging Face’s Actions workflows for machine learning on GitHub &#8617; . | These additional dependencies are defined here, which uses the “jekyll build” command to add ruby dedpendencies from the Gemfile located at the root of the repo. Additionally, this docker image is built by another Action workflow defined here. &#8617; . |",
            "url": "https://donatasrep.github.io/donatas.repecka/actions/markdown/2020/03/06/fastpages-actions.html",
            "relUrl": "/actions/markdown/2020/03/06/fastpages-actions.html",
            "date": " • Mar 6, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Introducing fastpages",
            "content": ". We are very pleased to announce the immediate availability of fastpages. fastpages is a platform which allows you to create and host a blog for free, with no ads and many useful features, such as: . Create posts containing code, outputs of code (which can be interactive), formatted text, etc directly from Jupyter Notebooks; for instance see this great example post from Scott Hawley. Notebook posts support features such as: Interactive visualizations made with Altair remain interactive. | Hide or show cell input and output. | Collapsable code cells that are either open or closed by default. | Define the Title, Summary and other metadata via a special markdown cells | Ability to add links to Colab and GitHub automatically. | . | Create posts, including formatting and images, directly from Microsoft Word documents. | Create and edit Markdown posts entirely online using GitHub&#39;s built-in markdown editor. | Embed Twitter cards and YouTube videos. | Categorization of blog posts by user-supplied tags for discoverability. | ... and much more | . fastpages relies on Github pages for hosting, and Github Actions to automate the creation of your blog. The setup takes around three minutes, and does not require any technical knowledge or expertise. Due to built-in automation of fastpages, you don&#39;t have to fuss with conversion scripts. All you have to do is save your Jupyter notebook, Word document or markdown file into a specified directory and the rest happens automatically. Infact, this blog post is written in a Jupyter notebook, which you can see with the &quot;View on GitHub&quot; link above. . fast.ai have previously released a similar project called fast_template, which is even easier to set up, but does not support automatic creation of posts from Microsoft Word or Jupyter notebooks, including many of the features outlined above. . Because fastpages is more flexible and extensible, we recommend using it where possible. fast_template may be a better option for getting folks blogging who have no technical expertise at all, and will only be creating posts using Github&#39;s integrated online editor. . Setting Up Fastpages . The setup process of fastpages is automated with GitHub Actions, too! Upon creating a repo from the fastpages template, a pull request will automatically be opened (after ~ 30 seconds) configuring your blog so it can start working. The automated pull request will greet you with instructions like this: . . All you have to do is follow these instructions (in the PR you receive) and your new blogging site will be up and running! . Jupyter Notebooks &amp; Fastpages . In this post, we will cover special features that fastpages provides for Jupyter notebooks. You can also write your blog posts with Word documents or markdown in fastpages, which contain many, but not all the same features. . Options via FrontMatter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . All of the above settings are enabled in this post, so you can see what they look like! . the summary field (preceeded by &gt;) will be displayed under your title, and will also be used by social media to display as the description of your page. | toc: setting this to true will automatically generate a table of contents | badges: setting this to true will display Google Colab and GitHub links on your blog post. | comments: setting this to true will enable comments. See these instructions for more details. | author this will display the authors names. | categories will allow your post to be categorized on a &quot;Tags&quot; page, where readers can browse your post by categories. | . Markdown front matter is formatted similarly to notebooks. The differences between the two can be viewed on the fastpages README. . Code Folding . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . If you want to completely hide cells (not just collapse them), read these instructions. . Interactive Charts With Altair . Interactive visualizations made with Altair remain interactive! . We leave this below cell unhidden so you can enjoy a preview of syntax highlighting in fastpages, which uses the Dracula theme. . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;IMDB_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget IMDB_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | 6.1 | . 1 First Love, Last Rites | 10876.0 | 300000.0 | 6.9 | . 2 I Married a Strange Person | 203134.0 | 250000.0 | 6.8 | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | NaN | . 4 Slam | 1087521.0 | 1000000.0 | 3.4 | . Other Features . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Images w/Captions . You can include markdown images with captions like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Of course, the caption is optional. . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . More Examples . This tutorial contains more examples of what you can do with notebooks. . How fastpages Converts Notebooks to Blog Posts . fastpages uses nbdev to power the conversion process of Jupyter Notebooks to blog posts. When you save a notebook into the /_notebooks folder of your repository, GitHub Actions applies nbdev against those notebooks automatically. The same process occurs when you save Word documents or markdown files into the _word or _posts directory, respectively. . We will discuss how GitHub Actions work in a follow up blog post. . Resources &amp; Next Steps . We highly encourage you to start blogging with fastpages! Some resources that may be helpful: . fastpages repo - this is where you can go to create your own fastpages blog! | Fastai forums - nbdev &amp; blogging category. You can ask questions about fastpages here, as well as suggest new features. | nbdev: this project powers the conversion of Jupyter notebooks to blog posts. | . If you end up writing a blog post using fastpages, please let us know on Twitter: @jeremyphoward, @HamelHusain. .",
            "url": "https://donatasrep.github.io/donatas.repecka/fastpages/jupyter/2020/02/21/introducing-fastpages.html",
            "relUrl": "/fastpages/jupyter/2020/02/21/introducing-fastpages.html",
            "date": " • Feb 21, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://donatasrep.github.io/donatas.repecka/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://donatasrep.github.io/donatas.repecka/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Microsoft Word Example Post",
            "content": "When writing a blog post with Microsoft Word – the filename becomes the title. In this case the file name is “2020-01-01-Microsoft-Word-Example-Post.docx”. . There is minimal support for Word documents in fastpages compared to Jupyter notebooks. Some known limitations: . alt text in Word documents are not yet supported by fastpages, and will break links to images. . | You can only specify front matter for Word documents globally. See the README for more details. . | . For greater control over the content produced from Word documents, you will need to convert Word to markdown files manually. You can follow the steps in this blog post, which walk you through how to use pandoc to do the conversion. Note: If you wish to customize your Word generated blog post in markdown, make sure you delete your Word document from the _word directory so your markdown file doesn’t get overwritten! . If your primary method of writing blog posts is Word documents, and you plan on always manually editing Word generated markdown files, you are probably better off using fast_template instead of fastpages. . The material below is a reproduction of this blog post, and serves as an illustrative example. . Maintaining a healthy open source project can entail a huge amount of toil. Popular projects often have orders of magnitude more users and episodic contributors opening issues and PRs than core maintainers capable of handling these issues. . Consider this graphic prepared by the NumFOCUS foundation showing the number of maintainers for three widely used scientific computing projects: . . We can see that across these three projects, there is a very low ratio maintainers to users. Fixing this problem is not an easy task and likely requires innovative solutions to address the economics as well as tools. . Due to its recent momentum and popularity, Kubeflow suffers from a similar fate as illustrated by the growth of new issues opened: . . Source: “TensorFlow World 2019, Automating Your Developer Workflow With ML” . Coincidentally, while building out end to end machine learning examples for Kubeflow, we built two examples using publicly available GitHub data: GitHub Issue Summarization and Code Search. While these tutorials were useful for demonstrating components of Kubeflow, we realized that we could take this a step further and build concrete data products that reduce toil for maintainers. . This is why we started the project kubeflow/code-intelligence, with the goals of increasing project velocity and health using data driven tools. Below are two projects we are currently experimenting with : . Issue Label Bot: This is a bot that automatically labels GitHub issues using Machine Learning. This bot is a GitHub App that was originally built for Kubeflow but is now also used by several large open source projects. The current version of this bot only applies a very limited set of labels, however we are currently A/B testing new models that allow personalized labels. Here is a blog post discussing this project in more detail. . | Issue Triage GitHub Action: to compliment the Issue Label Bot, we created a GitHub Action that automatically adds / removes Issues to the Kubeflow project board tracking issues needing triage. . | Together these projects allow us to reduce the toil of triaging issues. The GitHub Action makes it much easier for the Kubeflow maintainers to track issues needing triage. With the label bot we have taken the first steps in using ML to replace human intervention. We plan on using features extracted by ML to automate more steps in the triage process to further reduce toil. . Building Solutions with GitHub Actions . One of the premises of Kubeflow is that a barrier to building data driven, ML powered solutions is getting models into production and integrated into a solution. In the case of building models to improve OSS project health, that often means integrating with GitHub where the project is hosted. . We are really excited by GitHub’s newly released feature GitHub Actions because we think it will make integrating ML with GitHub much easier. . For simple scripts, like the issue triage script, GitHub actions make it easy to automate executing the script in response to GitHub events without having to build and host a GitHub app. . To automate adding/removing issues needing triage to a Kanban board we wrote a simple python script that interfaces with GitHub’s GraphQL API to modify issues. . As we continue to iterate on ML Models to further reduce toil, GitHub Actions will make it easy to leverage Kubeflow to put our models into production faster. A number of prebuilt GitHub Actions make it easy to create Kubernetes resources in response to GitHub events. For example, we have created GitHub Actions to launch Argo Workflows. This means once we have a Kubernetes job or workflow to perform inference we can easily integrate the model with GitHub and have the full power of Kubeflow and Kubernetes (eg. GPUs). We expect this will allow us to iterate much faster compared to building and maintaining GitHub Apps. . Call To Action . We have a lot more work to do in order to achieve our goal of reducing the amount of toil involved in maintaining OSS projects. If your interested in helping out here’s a couple of issues to get started: . Help us create reports that pull and visualize key performance indicators (KPI). https://github.com/kubeflow/code-intelligence/issues/71 . We have defined our KPI here: issue #19 | . | Combine repo specific and non-repo specific label predictions: https://github.com/kubeflow/code-intelligence/issues/70 . | . In addition to the aforementioned issues we welcome contributions for these other issues in our repo. .",
            "url": "https://donatasrep.github.io/donatas.repecka/2020/01/01/Microsoft-Word-Example-Post.html",
            "relUrl": "/2020/01/01/Microsoft-Word-Example-Post.html",
            "date": " • Jan 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am CTO co-founder of Biomatter Designs. Currently working on developing deep learning based tools for various aspects of protein engineering. . LinkedIn .",
          "url": "https://donatasrep.github.io/donatas.repecka/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://donatasrep.github.io/donatas.repecka/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}