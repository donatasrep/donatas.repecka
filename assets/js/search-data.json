{
  
    
        "post0": {
            "title": "Performance Analysis -  Pure Python",
            "content": "Introduction . In the previous post I have compared various languages and libraries in terms of their speed. This notebook contains the code used in the comparison as well as some details about the choices made to improve the performance of pure python implementation. . Setup . . # ! pip install pandas . Getting data . I will cheat here and use pandas to help me to read the file. . import pandas as pd . def get_data(path): fasta_df = pd.read_csv(path, sep=&quot; n&quot;, lineterminator=&quot;&gt;&quot;, index_col=False, names=[&#39;id&#39;, &#39;seq&#39;]) return fasta_df.seq.to_list() . seqs = get_data(&#39;../data/picked_msa.fasta&#39;) . Python naive implementation . Just to remind the pseudo code looks like this: . for seq1 in seqs: for seq2 in seqs: if count_mathes(seq1, seq2) &gt; threshold: weight +=1 meff += 1/weight meff = meff/(len(seq1)^0.5) . It translates into python relatively easy . def get_nf_python(seqs, threshold=0.8): n_seqs, seq_len = len(seqs), len(seqs[0]) meff = 0 for seq1 in seqs: c = 0 for seq2 in seqs: identity = 0 for p in range(seq_len): identity = identity + int(seq1[p] == seq2[p]) is_more = int((identity/seq_len) &gt; threshold) c = c + is_more meff = meff + 1/c return meff/(seq_len**0.5) . This is O((n^2)*l) complexity (where l is length in the sequence) as we need to go through the n sequences for every n sequences (pairwise operation) and compare each element in the sequence. This is not ideal and the scale of such an algorithm is poor. If interested you can read more about this here. . %%timeit -n 3 -r 3 get_nf_python(seqs[:100]) . 1.86 s ± 46.6 ms per loop (mean ± std. dev. of 3 runs, 3 loops each) . get_nf_python(seqs[:100]) . 0.18006706787628668 . Fortunately, the algorithm can be improved by exploiting the fact that number of matches are the same between seq1, seq2 and sequences seq2, seq1 (the pairwise matrix is symmetric). . def get_nf_python_v2(seqs, threshold=0.8): n_seqs, seq_len = len(seqs), len(seqs[0]) is_same_cluster = [[ 1 for _ in range(n_seqs)] for _ in range(n_seqs)] meff = 0 for i in range(n_seqs): for j in range(i+1,n_seqs): identity = 0 for p in range(seq_len): identity = identity + int(seqs[j][p] == seqs[i][p]) is_more = int((identity/seq_len) &gt; threshold) is_same_cluster[i][j]=is_more is_same_cluster[j][i]=is_more c = sum(is_same_cluster[i]) meff = meff + 1/c return meff/(seq_len**0.5) . %%timeit -n 3 -r 3 get_nf_python_v2(seqs[:100]) . 1.11 s ± 8.22 ms per loop (mean ± std. dev. of 3 runs, 3 loops each) . get_nf_python_v2(seqs[:100]) . 0.18006706787628668 . Although, technically, this version is still O((n^2)*l), but in practise the speed improvement will be substantial (in this case, improvement is roughly 40%). Also, it is possible to break the last inner loop earlier, however, I did not see any performance gain (cost if statement offset the gain from early stopping). . Note that I used only 1% of the data. Running on these algorithms on full data set would take too long and hence not recommended. .",
            "url": "https://donatasrep.github.io/donatas.repecka/performance/2021/05/08/performance-pure-python.html",
            "relUrl": "/performance/2021/05/08/performance-pure-python.html",
            "date": " • May 8, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Performance Analysis -  Numpy",
            "content": "Introduction . In the previous post I have compared various languages and libraries in terms of their speed. This notebook contains the code used in the comparison as well as some details about the choices made to improve the performance of numpy implementation. . Setup . . # ! pip install pandas . Getting data . import pandas as pd . def get_data(path): fasta_df = pd.read_csv(path, sep=&quot; n&quot;, lineterminator=&quot;&gt;&quot;, index_col=False, names=[&#39;id&#39;, &#39;seq&#39;]) return fasta_df.seq.to_numpy(dtype=str) . seqs = get_data(&#39;../data/picked_msa.fasta&#39;) . Numpy implementation . Just to remind the pseudo code looks like this: . for seq1 in seqs: for seq2 in seqs: if count_mathes(seq1, seq2) &gt; threshold: weight +=1 meff += 1/weight meff = meff/(len(seq1)^0.5) . This Numpy implementation is based on pure python implementation which can be found here. . The main differences are: . uses numpy arrays and operators. | sequences are in vectorised fashion using np.equal rather than looping over each element in the sequence. | sequences are converted to arrays of integers rather than characters. | . import numpy as np def get_nf_python_numpy(seqs, threshold=0.8): seqs = seqs.view(np.uint32).reshape(seqs.shape[0], -1) n_seqs = len(seqs) is_same_cluster = np.ones([n_seqs, n_seqs], np.bool_) for i in range(n_seqs): for j in range(i+1, n_seqs): identity = np.equal(seqs[i], seqs[j]).mean() is_more = np.greater(identity, threshold) is_same_cluster[i,j] = is_more is_same_cluster[j,i] = is_more meff = 1.0/np.sum(is_same_cluster,1) return meff.sum()/(seqs.shape[-1]**0.5) . %%timeit -n 3 -r 3 get_nf_python_numpy(seqs[:100]) . 132 ms ± 7.67 ms per loop (mean ± std. dev. of 3 runs, 3 loops each) . get_nf_python_numpy(seqs[:100]) . 0.18006706787628665 . Although performance gain is substantial, I still have two loops which is one of the first things you want to tackle when trying to improve algorithm speed. In practise, you want to use vectorization to take advantage of CPU ability to perform the same operation on vectors. . Ideally, I would like to do all to all pairwise comparison in one vectorized operation. Unfortunately, this is not a viable option due to memory constraints . Such operation will require n * n * l size matrix. In my case, that would result into ~70GB size matrix (n = 10000, l = 683, boolean uses 1 byte) . $$10000^2 *683 = 68.3*10^9 bytes = 68.3 gigabytes$$ . To be able to run such an algorithm on an everyday computer, the memory requirement has to be lower. Usually, this can be solved with batching, taking only a portion of data at the time. In this case, I will perform a one-to-all comparison at the time. This will require n*l memory which is 10000 (~7MB) times less than for the all-to-all version. . Note, I still can exploit the fact that the matrix of identity is symmetric which means that only the first iteration has to be done with n-1 sequences, the second sequence needs only be compared with n-2 and so on. . def get_one_to_all_comparison(seqs, threshold=0.8): pairwise_id = np.equal(seqs[1:], seqs[0].T).mean(1) return pairwise_id &gt; threshold . def get_nf_numpy_v2(seqs): seqs = seqs.view(np.uint32).reshape(seqs.shape[0], -1) n_seqs, seq_len = seqs.shape is_same_cluster = np.ones([n_seqs, n_seqs],np.bool_) for i in range(n_seqs-1): out = get_one_to_all_comparison(seqs[i:]) is_same_cluster[i, i+1:] = out is_same_cluster[i+1:, i] = out s = 1.0/is_same_cluster.sum(1) return s.sum()/(seq_len**0.5) . %%timeit -n 1 -r 1 get_nf_numpy_v2(seqs[:100]) . 26.1 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each) . get_nf_numpy_v2(seqs[:100]) . 0.18006706787628665 . By using numpy, the algorithm&#39;s runtime decreases significantly. The gain on 1% of the data is roughly 100 times (might differ depending on the hardware). The improvement will scale with the amount of data that is being processed so on the data set the gain will be even larger. . Multiprocessing . There is a limit to how much an algorithm can be improved. However, more speed can be gained by simply exploiting the sheer size of infrastructure, i.e. multiple CPUs/machines. Unfortunately, this rarely happens automatically, unless you use libraries that have this ability built-in. Although Numpy has some functionality that runs in parallel, in this case some additional work is required. . In this case, sequence comparison does not need to be performed sequentially, thus can be parallelised. . Some effort were made to make code below work on Windows machines. As a result, we have to create a seperate file for worker code. More information here . . import multiprocessing from worker import * def get_nf_numpy_mp(seqs, threads=None): seqs = seqs.view(np.uint32).reshape(seqs.shape[0], -1) n_seqs, seq_len = seqs.shape with multiprocessing.Pool(threads, initializer=init, initargs=[seqs]) as pool: results = pool.map(get_one_to_all_comparison_global, range(n_seqs-1)) is_same_cluster = np.ones([n_seqs, n_seqs],np.bool_) for out in results: i = n_seqs - out.shape[0] - 1 is_same_cluster[i, i+1:] = out is_same_cluster[i+1:, i] = out meff = 1.0/is_same_cluster.sum(1) return meff.sum()/(seq_len**0.5) . %%timeit -n 1 -r 1 if __name__ == &quot;__main__&quot;: meff = get_nf_numpy_mp(seqs) print(meff) . 29.466257863621866 46.5 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each) . %%timeit -n 1 -r 1 meff = get_nf_numpy_v2(seqs[:10000]) print(meff) . 29.466257863621866 1min 35s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each) . The benefit of multiprocessing will be determined by hardware. However, it is important to remember that there is an overhead of distributing the work across CPUs. As a result, if the algorithm takes a couple of seconds, multiprocessing might make it even slower. On the other hand, really big tasks can be speeded up by nearly the factor of number of threads on the machine where overhead becomes negligible. .",
            "url": "https://donatasrep.github.io/donatas.repecka/performance/2021/05/08/performance-numpy.html",
            "relUrl": "/performance/2021/05/08/performance-numpy.html",
            "date": " • May 8, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Performance Analysis -  Numba",
            "content": "Introduction . In the previous post I have compared various languages and libraries in terms of their speed. This notebook contains the code used in the comparison as well as some details about the choices made to improve the performance of numba implementation. . From Numba website: &quot;Numba translates Python functions to optimized machine code at runtime using the industry-standard LLVM compiler library. Numba-compiled numerical algorithms in Python can approach the speeds of C or FORTRAN.&quot; . Setup . . # ! pip install pandas # ! pip install numba . Getting data . import pandas as pd . def get_data(path): fasta_df = pd.read_csv(path, sep=&quot; n&quot;, lineterminator=&quot;&gt;&quot;, index_col=False, names=[&#39;id&#39;, &#39;seq&#39;]) return fasta_df.seq.to_numpy(dtype=str) . seqs = get_data(&#39;../data/picked_msa.fasta&#39;) . Just to remind the pseudo code looks like this: . for seq1 in seqs: for seq2 in seqs: if count_mathes(seq1, seq2) &gt; threshold: weight +=1 meff += 1/weight meff = meff/(len(seq1)^0.5) . As with Numpy and Python versions, we use the same input data. The code is closer to the version of pure Python because wrapping optimised Numpy code turned out to be slower. It seems that you are better off leaving all optimisation for Numba. . import numpy as np from numba import jit, prange . def get_nf_numba(seqs, threshold=0.8): seqs = seqs.view(np.uint32).reshape(seqs.shape[0], -1) n_seqs, seq_len = seqs.shape is_same_cluster = np.eye(n_seqs) for i in prange(n_seqs): c = 0 for j in prange(i+1, n_seqs): identity = np.equal(seqs[i], seqs[j]).mean() is_more = np.greater(identity, threshold) is_same_cluster[i,j] = is_more is_same_cluster[j,i] = is_more meff = 1.0/is_same_cluster.sum(1) return meff.sum()/(seq_len**0.5) . There are a couple of things that need to be done in order to utilise Numba fully. Firstly, Numba uses JIT -(just in time compilation)[https://en.wikipedia.org/wiki/Just-in-time_compilation]. Hence you need to wrap your functions with either @jit or ‘jit’ function. Note, the first run of the wrapped function will be slower as Numba needs to compile code. Secondly, there is the nopython option that bypasses the Python interpreter. It has its own down sides that allows code to run faster. . fn = jit(get_nf_numba, nopython=True,parallel=False) fn(seqs[:100]) . 0.18006706787628668 . %%timeit -n 3 -r 3 fn(seqs[:100]) . 11 ms ± 642 µs per loop (mean ± std. dev. of 3 runs, 3 loops each) . Another really nice feature of Numba is that it allows to parallelise code with one single option as you can see below. . fn = jit(get_nf_numba, nopython=True,parallel=True) fn(seqs[:100]) . 0.18006706787628668 . %%timeit -n 3 -r 3 fn(seqs[:100]) . 6.11 ms ± 3.13 ms per loop (mean ± std. dev. of 3 runs, 3 loops each) . Finally, if precision is less important and can be sacrificed for extra speed, there is fastmathoption. From (Numba documentation)[https://numba.readthedocs.io/en/stable/user/performance-tips.html?highlight=fastmath#fastmath]: . “In certain classes of applications strict IEEE 754 compliance is less important. As a result it is possible to relax some numerical rigour with view of gaining additional performance. The way to achieve this behaviour in Numba is through the use of the fastmath keyword argument” . fn = jit(get_nf_numba, nopython=True,parallel=True, fastmath=True) fn(seqs[:100]) . 0.18006706787628665 . %%timeit -n 3 -r 3 fn(seqs[:100]) . The slowest run took 6.90 times longer than the fastest. This could mean that an intermediate result is being cached. 4.31 ms ± 3.63 ms per loop (mean ± std. dev. of 3 runs, 3 loops each) .",
            "url": "https://donatasrep.github.io/donatas.repecka/performance/2021/05/08/performance-numba.html",
            "relUrl": "/performance/2021/05/08/performance-numba.html",
            "date": " • May 8, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Performance comparison of the number of effective sequences algorithm",
            "content": "Performance comparison of the number of effective sequences algorithm . Introduction . From time to time I hear something along the lines ‘we should rewrite this in C to improve performance’. I always was sceptical about this idea, but I have not had any practical experience to be able to give a concrete example. During the process of making this post I explored the performance difference of one algorithm written in different ways including C and C++. The main questions I was intrigued about were: . How much faster is C/C++ in practise if I were to rewrite code? | Does the gain justify the effort? | Are any of the Python performance libraries like Cython or Numba useful in practise? | . Note: a one example is not enough to draw any solid conclusions, but when it resonates with what you heard before that is at least reassuring. . I hope this will be yet another performance comparison which in combination with all other ones will be helpful to make a decision in your particular case. . Algorithm to be tested . The algorithm used to evaluate performance is the number of effective sequences (Nf) in the MSA. The algorithm is quite simple: the input is a list of equal length sequences, we need to calculate how many effectively unique sequences there are (a single number). In order to do so, firstly we need to calculate pairwise identities (what percentage of sequence is matching) and if the identity is higher than chosen threshold, we cluster them together. The inverse of cluster size gives you a weight of the sequence. A sum of weights is the Nf. Intuitively, if all sequences are similar, you will get one cluster, hence the Nf will be 1, at the other end of the spectrum, if all sequences are completely different, cluster sizes will be 1, hence the Nf will be equal to the number of sequences. We have the normalization term at the end to normalize by sequence length. . Pseudo code looks like this: . for seq1 in seqs: for seq2 in seqs: if count_mathes(seq1, seq2) &gt; threshold: weight +=1 meff += 1/weight meff = meff/(len(seq1)^0.5) . Code complexity is O((n^2)*l) complexity (where l is length in the sequence) which means it does not scale well. Nevertheless there are ways to make it faster. . Setup . To perform testing I used the MSA which consists of 10000 sequences of length 683. The hardware used was: Intel(R) Core(TM) i7-10750H CPU @ 2.60GHz 16GB RAM + Nvidia 1650GTX. . Note: the results might vary wildly depending of the hardware you have. It is good to keep that in mind when analysing results. . Baselines . For baseline I used C and C++ implementations. As I do not consider myself an expert in C or C++ I borrowed existing implementations from existing sources: . C++ implementation was extracted from DeepMSA package. No changes have been made to it. | C implementation wast taken from Prody. Once again the code was run as it is. | . Results . In this section I display the results I got on my hardware. The implementations are extracted to separate notebooks for making this post bearable. While code might not be optimal, I did spend a significant amout of time trying to optimise the code. . Single threaded . Code Threads Time . C++ | 1 | 02:26.86 | . C | 1 | 00:41.35 | . Pure Python | 1 | ~7 hours* | . Numpy | 1 | 00:32.51 | . Numba | 1 | 00:26.63 | . Cython | 1 | 00:32.43 | . Julia | 1 | 04:51.00** | . * Pure Python times were estimated on the timings I got on 100 sequences. . ** I am quite sure that Julia can be faster and the developer is to blame for its poor performance. . The very first thing, Python is slow, very slow and if I would just compare Python vs C/C++ that would be the end of the story. However, the main strength of Python is its ecosystem. There are a lot of optimised libraries you can use (just to be clear, I have not tested all of them, only the ones I thought to be good to test) which improves performance drastically. In my case, Numpy, Cython and Numba implementations were faster than the C/C++, Numba being the fastest one (I have opted to use Fastmath option which sacrifices precision for speed, more about this in the notebook). In general, it seems that any optimized library could achieve C/C++ like performance. . Multi threaded . While you could try to optimize the performance of the algorithm till perfection, in practise relative optimized code that scales will be way faster than optimal one on single thread. You can see that in the table below. . Code Threads Time . Numpy | 12 | 00:07.11 | . Numba | 12 | 00:05.12 | . Tensroflow | 12 | 00:39.88 | . Pytorch | 12 | 00:39.14 | . Some notes: C and C++ implementation did not have support multi threaded execution. Numpy version utilized the multiprocessing library to do multithreading, while numba has built-in support for that. I skipped Julia and Cython due to time constraints. I have also introduced Tensorflow and Pytorch as my work is around developing deep learning models and I try to compare those frameworks whenever I have a chance. While Numba and Numpy performance was as expected - much faster on multiple threads (just to note: there is some overhead due to multithreading, but it is still worth scaling up), Tensorflow and Pytorch were both quite slow in comparison (I have to admit, not quite sure why). . With accelerator (GPU) . Nowadays, if you really want to go fast, you use accelerators such as GPU and TPU. While it requires to convert your code into the form that GPU likes (pretty much matrix multiplication), the gains sometimes are incredible. Even running on a mediocre GPU (Nvidia 1650GTX), I managed to have the best performance. What is more, Jax performance seems like out of this world, but I will cover the details of the implementations separately. . Code Threads Time . Tensorflow | 1 | 00:06.10 | . Pytorch | 1 | 00:08.05 | . JAX | 1 | 00:00.13 | . Conclusion . During this experiment I have not found anything that would be ground breaking or new. Nevertheless, I the findings are extremelly useful for myself. The main takeaways are: . Pure Python is slow. | Optimised Python libraries can give you approximately C/C++ performance. | Scaling to multiple threads or using GPU will give you the best performance. | Yet another thing which is not visible in the tables and is often overlooked is the solution itself. I have written some pieces several times to get these results (improving performance substantially). | . And if you were to ask me whether it is worth going to C/C++ for the performance reasons, I would say: unless you really need to squeeze every split of the second, you can stay in Python and be at least competitive with C/C++ performance, not to mention development/maintenance side of things. .",
            "url": "https://donatasrep.github.io/donatas.repecka/performance/2021/04/27/Performance-comparison.html",
            "relUrl": "/performance/2021/04/27/Performance-comparison.html",
            "date": " • Apr 27, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Hands-on Spectral Normalization",
            "content": "Spectral normalization is a widely used technique to stabilize and improve the training of Generative adversarial networks. In nutshell, this normalization technique allows the measurement of meaningful distance between real and generated examples using discriminator. This measure is then used to train both, the discriminator and the generator. . Introduction . In my previous post I have walked through changes proposed in WGAN paper skipping an import piece about Lipschitz constraint. In this post I will discuss the most effective (up to this day) technique to satisfy this constraint. . As we talked in previous post, work of the authors of WGAN paper proposed to use the distance between the outputs of the discriminator as a proxy for distances between distributions. To put it simply, we want to measure how far two distributions are from each other. In order to be able to use the discriminator output as a proxy, we need to bound discriminator to be Lipschitz constrained. . Spectral Normalization . Interestingly, the authors of WGAN paper turned to the AI scientific community for the best approach. And a year later the paper called Spectral Normalization for Generative Adversarial Networks came out with a proposed solution. In comparison to previous attempts, this solution was superior due to its efficiency. So let&#39;s dig into it. . According to the paper, the Lipschitz constraint of the layer can be satisfied by dividing the weights of the layer by the largest singular value of the same weights. A really nice explanation and proof can be found here. Mathematically it looks like this: . $$W=W/ sigma(W)$$ . where $ sigma(W)$ is the largest singular value of the weights $W$. . An important observation by the authors of the paper is that if we do it for all layers, we will have a network that satisfies Lipschitz constraint. . Sounds simple enough, let&#39;s try it out. . import numpy as np . Let&#39;s take a convolutional filter of kernel size 3x3, 256 input channels and 512 output channels. Note: we reshaped the kernel so that we could calculate singular values. . W = np.random.normal(size = [3,3, 256, 512]).reshape([-1,512]) W.shape . (2304, 512) . Let&#39;s use numpy library to find singular values. numpy.linalg.svd can do that for us. The second output of numpy.linalg.svd function returns all singular values, we just need a maximum of that. . %%timeit s = np.linalg.svd(W, full_matrices=True)[1].max() s . 1 loop, best of 3: 1.02 s per loop . That is quite simple, however, it takes 1 second to calculate those values. Let&#39;s assume that we have 30 layers in the network, that would mean extra 30 seconds for each training step. Well, that is not what we could call efficient. . from scipy.sparse.linalg import svds . There is an alternative. scipy.sparse.linalg.svds function returns only the number k of the largest singular values. . %%timeit s = svds(W, k=1)[1] s . 10 loops, best of 3: 72 ms per loop . This is much faster: ~74ms, however for 30 layers we will slow down the training process by 2 seconds per step. Still not ideal. . Luckly, The authors of the paper proposed an alternative solution to get the largest singular value. This technique is known as power iteration. This looks like this: $$ v = W^ intercal u / ||W^ intercal u||_2$$ . $$ u = W v / ||W v||_2$$ . $$ W = W / u^ intercal W v$$ . Let&#39;s see if we can implement it. . $u$ needs to be sampled from an isotropic distribution at the beginning and it&#39;s dimensions should match the dimensions of the number of output channels. . u = np.random.normal(scale=0.2, size=[512]) u.shape . (512,) . First row of the equation . Note: we swapped u and W just for the sake of simplicity when implementing in numpy . v = u@W.T/np.linalg.norm(u@W.T, 2) v.shape . (2304,) . Second row of the equation . u = v@W / np.linalg.norm(v@W, 2) u.shape . (512,) . Third row of the equation . sigma = v@W@u.T sigma . 53.890708570486424 . Let&#39;s put everything together . def get_largets_singular_value(u): v = u@W.T/np.linalg.norm(u@W.T, 2) u = v@W / np.linalg.norm(v@W, 2) sigma = v@W@u.T return sigma, u . %%timeit sigma, _ = get_largets_singular_value(u.copy()) . 100 loops, best of 3: 2.06 ms per loop . print(&quot;Answer from numpy:&quot;, svds(W, k=1)[1].squeeze()) print(&quot;Power iteration outcome&quot;, sigma) . Answer from numpy: 70.21844081626173 Power iteration outcome 69.4230148155534 . Well, it is fast, ~40x faster, but it is not accurate. Well, that is expected because power iteration only approximates the largest singular value. The more iterations you perform, the more accurate approximation you would get. . Let’s see if that is true. . u_copy = u.copy() actual = svds(W, k=1)[1].squeeze() for i in range(20): sigma,u_copy = get_largets_singular_value(u_copy) print(&quot;Power iteration estimate: {:.2f} (actual: {:.2f})&quot;.format(sigma, actual)) . Power iteration estimate: 60.51 (actual: 70.22) Power iteration estimate: 63.85 (actual: 70.22) Power iteration estimate: 65.64 (actual: 70.22) Power iteration estimate: 66.69 (actual: 70.22) Power iteration estimate: 67.36 (actual: 70.22) Power iteration estimate: 67.82 (actual: 70.22) Power iteration estimate: 68.15 (actual: 70.22) Power iteration estimate: 68.40 (actual: 70.22) Power iteration estimate: 68.59 (actual: 70.22) Power iteration estimate: 68.74 (actual: 70.22) Power iteration estimate: 68.86 (actual: 70.22) Power iteration estimate: 68.97 (actual: 70.22) Power iteration estimate: 69.06 (actual: 70.22) Power iteration estimate: 69.13 (actual: 70.22) Power iteration estimate: 69.19 (actual: 70.22) Power iteration estimate: 69.25 (actual: 70.22) Power iteration estimate: 69.30 (actual: 70.22) Power iteration estimate: 69.35 (actual: 70.22) Power iteration estimate: 69.39 (actual: 70.22) Power iteration estimate: 69.42 (actual: 70.22) . This is indeed the case. However, doing power iteration 20 times gets us close to scipy performance. Fortunately, Spectral Normalization paper showed that you can do one or a small number of iterations per training step to get a good estimate throughout the entire training (doing incremental work every step). Hence, even using naive numpy implementation we can get to only extra 60ms (1.7ms*30) for each training iteration (assuming 30 layer network). . That does not sound too bad. . Note: keep in mind that these performance numbers are relative, they will depend on hardware. . We can try to port this to tensorflow and measure its performance. . import tensorflow as tf tf.config.list_physical_devices(&#39;GPU&#39;) . [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . @tf.function def get_largets_singular_value_tf(u, w): _v = tf.matmul(u, w, transpose_b=True) _v = tf.math.l2_normalize(_v) _u_m = tf.matmul(_v, w) _u = tf.math.l2_normalize(_u_m) sigma = tf.matmul(_u_m, _u, transpose_b=True) return sigma, _u . u_tf = tf.Variable(tf.random.normal(stddev=0.2, shape=[1,512]), trainable=False) w_tf = tf.Variable(tf.random.normal(shape = [3*3*256, 512]), trainable=False) . %%timeit sigma_tf, _ = get_largets_singular_value_tf(u_tf, w_tf) . The slowest run took 65.11 times longer than the fastest. This could mean that an intermediate result is being cached. 1000 loops, best of 3: 509 µs per loop . for i in range(20): sigma_tf, u_tf = get_largets_singular_value_tf(u_tf, w_tf) print(sigma_tf) . tf.Tensor([[69.43079]], shape=(1, 1), dtype=float32) . That is even better. 509µs with tensorflow implementation and executed on GPU (which usually is the case when you are training GANs). So if we assume 30 layers of similar size and one power iteration per training step, we only need ~15ms extra for the step. That is the reason why this technique is widely and successfully adopted. . In practice . Since we have all the pieces, we can create a layer wrapper to wrap any layer and perform power iteration on each feed forward pass. I have extended the code of here I have incorporated suggestions from the comments: . Turned off power iterations during the inherence. This makes sure that weights are not being changed doing inherence. | Used assigned operation instead of =. This speeds up the algorithm quite significantly (roughly by 35%). | . Additionally I have also added a hack to support mixed precision as well as the logic to support embedding layers. The final solution looks like this: . class SpectralNormalizationV2(tf.keras.layers.Wrapper): &quot;&quot;&quot; Attributes: layer: tensorflow keras layers (with kernel or embedding attribute) &quot;&quot;&quot; def __init__(self, layer, eps=1e-12, **kwargs): super(SpectralNormalizationV2, self).__init__(layer, name=layer.name + &quot;_sn&quot;, **kwargs) self.eps = eps self.is_embedding = isinstance(self.layer, tf.keras.layers.Embedding) def get_kernel_variable(self, attr=&#39;kernel&#39;): if not hasattr(self.layer, attr): raise ValueError(&#39;`SpectralNormalization` must wrap a layer that contains a `{}` for weights&#39;.format(attr)) return getattr(self.layer, attr) def build(self, input_shape=None): if not self.built: super(SpectralNormalizationV2, self).build(input_shape) if self.is_embedding: self.w = self.get_kernel_variable(&quot;embeddings&quot;) else: self.w = self.get_kernel_variable() self.autocast = hasattr(self.w, &quot;_variable&quot;) self.last_dim = self.w.shape[-1] self.u = self.add_weight(shape=[1, self.last_dim], initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02), name=&#39;sn_u&#39;, trainable=False, experimental_autocast=False) @tf.function def call(self, inputs, training=True): # Recompute weights for each training forward pass if training: self._compute_weights() output = self.layer(inputs, training=training) return output def _compute_weights(self): &quot;&quot;&quot;Generate normalized weights. This method will update the value of self.layer.kernel with the normalized value, so that the layer is ready for call(). &quot;&quot;&quot; if self.autocast: w = self.w._variable else: w =self.w w_reshaped = tf.reshape(w, [-1, self.last_dim]) _v = tf.matmul(self.u, w_reshaped, transpose_b=True) _v = tf.math.l2_normalize(_v, epsilon=self.eps) _u_m = tf.matmul(_v, w_reshaped) _u = tf.math.l2_normalize(_u_m, epsilon=self.eps) sigma = tf.matmul(_u_m, _u, transpose_b=True) self.u.assign(_u) self.w.assign(w / sigma) def compute_output_shape(self, input_shape): return self.layer.compute_output_shape(input_shape) . And the way you use is: . dense_sn = SpectralNormalizationV2(tf.keras.layers.Dense(units=100)) conv_sn = SpectralNormalizationV2(tf.keras.layers.Conv2D(filters=256, kernel_size=3)) emb_sn = SpectralNormalizationV2(tf.keras.layers.Embedding(input_dim=20, output_dim=100)) . I have also checked the performance in comparison to Pytorch implementation . Setup: . Batch size: 64 | Kernel of size :[16, 16, 256, 512] | Steps: 1000 | One power iteration per step | Hardware: NVIDIA V100 | . Results are: . Elapse time of Tensorflow SpectralNormalizationV2: 1.1081128120422363s | Elapse time of Pytorch official SpectralNorm implementation: 1.0783729553222656s | . Pytorch seems to be a little bit faster. I am not entirely sure whether it has to do with implementations of spectral normalization or just to the differences between Pytorch and Tensorflow. Regardless, those extra 30ms every 1000 steps should not be a game changer. . Summary . Spectral normalization is quite widely used in various implementations of GANs. One of the most famous applications is Biggan. All images below are generated by BigGAN architecture that uses Spectral normalization. This should highlight the importance of this advancement. . Throughout this post we focused on the implementation of power iteration proposed in Spectral Normalization for Generative Adversarial Networks paper. We have compared various methods to calculate the largest singular value and observed that the proposed power iteration method is the fastest and after multiple iterations it achieves reasonable accuracy. . .",
            "url": "https://donatasrep.github.io/donatas.repecka/2020/04/08/spectral-norm.html",
            "relUrl": "/2020/04/08/spectral-norm.html",
            "date": " • Apr 8, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Why we use WGAN?",
            "content": "Introduction . The original version of GAN was released in 2014. It was quite revolutionary and sparked the new line of research that is still very active. What is so revolutionary about GANs? The authors of the paper proposed to train two neural networks (discriminator and generator) stacked against each other in order to overcome the problem of having to create a loss function for the generator. . Now it seems rather obvious, however, at the time it was a wild idea. And even if it looked mathematically feasible, there was no indication that it could work in practise. Nevertheless, Ian Goodfellow and his colleagues demonstrated that such architecture can actually work even with resources at the time. See an examples from the same paper below (Note: images in yellow are the closest exmamples from training dataset) . Source: https://arxiv.org/pdf/1406.2661.pdf . In nutshell, generative adversarial networks consist of two networks: generator which generates data points and discriminator that evaluates whether input is coming from training dataset or is generated by generator. It is simple enough, however, it is quite important how the task of discriminator is formulated. That will be the main theme of this post. . Original discriminator objective . In the original version of GAN measure of real and generated examples was formulated as binary classification: is it real or is it generated. Let&#39;s look at the simple case, where we have true distribution (in other words real examples) that consists of 0&#39;s. A perfect discriminator will return 1 when input to the discriminator is 0 and output of the discriminator would be 0 otherwise. Despite the fact that discriminator is doing a great job at discriminating between real and generated, however, the generator does not receive meaningful direction for improving. For example, generator output of 0.1 and 1 will be scored the same, even though 0.1 is much closer to real distribution. Let&#39;s see this in action. . import tensorflow as tf import numpy as np from matplotlib import pyplot as plt np.set_printoptions(suppress=False, precision=5) . Discriminator . def d_loss(d_score_z, d_score_r): batch_size = d_score_z.shape[0] d_loss_z =tf.keras.losses.binary_crossentropy(tf.zeros(shape = [batch_size,1], dtype=tf.float32), d_score_z) d_loss_r =tf.keras.losses.binary_crossentropy(tf.ones(shape = [batch_size,1], dtype=tf.float32), d_score_r) return tf.reduce_mean(d_loss_z + d_loss_r) def define_discriminator(act=&quot;sigmoid&quot;): d_optimizer = tf.keras.optimizers.Adam(learning_rate=0.05) # Discriminator discriminator = tf.keras.models.Sequential() discriminator.add(tf.keras.layers.Dense(32, activation=&quot;relu&quot;)) discriminator.add(tf.keras.layers.Dense(32, activation=&quot;relu&quot;)) discriminator.add(tf.keras.layers.Dense(1, activation=act)) return discriminator, d_optimizer . def train_discriminator(discriminator, d_optimizer, d_loss, batch_size = 64): real_data = tf.zeros([batch_size, 1]) # Train discriminator till perfection for i in range(1000): with tf.GradientTape() as tape: z = tf.random.normal(shape=[batch_size, 1]) d_score_z, d_score_r = discriminator(z), discriminator(real_data) total_loss = d_loss(d_score_z, d_score_r) if i % 100 == 0: print(&quot;Total loss:&quot;, total_loss.numpy(), &quot;Scores for real:&quot;, d_score_r.numpy().mean(), &quot;Scores for generated:&quot;, d_score_z.numpy().mean()) grads = tape.gradient(total_loss, discriminator.trainable_variables) d_optimizer.apply_gradients(zip(grads, discriminator.trainable_variables)) . Disclaimer: we are creating an artificial situation just to demonstrate the underlying issue with using binary classification as a loss for discriminator. In reality, we would not train discriminators in this fashion. . discriminator, d_optimizer = define_discriminator() train_discriminator(discriminator, d_optimizer, d_loss) . Total loss: 1.2753901 Scores for real: 0.5 Scores for generated: 0.43882957 Total loss: 0.07295962 Scores for real: 0.9781779 Scores for generated: 0.015023634 Total loss: 0.014039941 Scores for real: 0.986065 Scores for generated: 7.0891906e-06 Total loss: 0.021772312 Scores for real: 0.9784628 Scores for generated: 5.510665e-11 Total loss: 0.075927064 Scores for real: 0.9949504 Scores for generated: 0.015457453 Total loss: 0.11446469 Scores for real: 0.9977589 Scores for generated: 0.038589276 Total loss: 0.00037504316 Scores for real: 0.99962485 Scores for generated: 4.787702e-14 Total loss: 0.07557466 Scores for real: 0.9988571 Scores for generated: 0.017265305 Total loss: 0.0038048518 Scores for real: 0.9962043 Scores for generated: 2.0079315e-06 Total loss: 0.0021338563 Scores for real: 0.9978684 Scores for generated: 6.928249e-14 . So we have got a nearly perfect discriminator that gives 1 for real distribution and 0 for anything else. Let&#39;s train the generator using such discriminator. . Generator . Here we will try to train the generator using perfect discriminator. Again, you would not do that in practise. . def g_loss(x, discriminator=discriminator): return tf.reduce_mean(1.0-discriminator(x)) def train_generator(g_loss, batch_size=64): g_optimizer = tf.keras.optimizers.Adam(learning_rate=0.05) generator = tf.keras.layers.Dense(1, activation=&#39;tanh&#39;) for i in range(1000): with tf.GradientTape() as tape: z = tf.random.normal(shape=[batch_size, 1]) gen = generator(z) loss = g_loss(gen) if i % 100 == 0: print(&quot;Loss:&quot;, loss.numpy(), &quot;Generator output: &quot;,tf.reduce_mean(tf.abs(gen)).numpy()) grads = tape.gradient(loss, generator.trainable_variables) g_optimizer.apply_gradients(zip(grads, generator.trainable_variables)) t = plt.scatter(np.zeros(z.shape), z.numpy(), label=&quot;True&quot;) g = plt.scatter(gen.numpy(), z.numpy(), alpha=0.5) plt.xlim(-1, 1) plt.legend((t,g),(&quot;True distribution&quot;, &quot;Generated distribution&quot;)) . train_generator(g_loss) . Loss: 0.98446023 Generator output: 0.5757177 Loss: 1.0 Generator output: 0.62681496 Loss: 0.99996835 Generator output: 0.68048704 Loss: 1.0 Generator output: 0.6950605 Loss: 1.0 Generator output: 0.7098987 Loss: 1.0 Generator output: 0.7288726 Loss: 1.0 Generator output: 0.9237368 Loss: 1.0 Generator output: 0.9254451 Loss: 1.0 Generator output: 0.922577 Loss: 1.0 Generator output: 0.923764 . It does not converge, the generator does not learn to produce 0&#39;s and loss does not decrease. We could find a way to train so that it converges, however, ideally we would want something that would be able to converge in this situation as well. . This issue was described in famous Wasserstein GAN paper. In this paper authors proposed to change binary classification to something that measures distance between generated outputs and real examples. . Let&#39;s try it out with our simplified example. . WGAN . A very naive approach would be to measure Euclidean distance between real and generated distributions. Let’s see if that helps. . def euclidean_distance(x): return tf.reduce_mean(tf.norm(x, &#39;euclidean&#39;)) train_generator(euclidean_distance) . Loss: 1.8566905 Generator output: 0.19034971 Loss: 0.06090928 Generator output: 0.005873415 Loss: 0.05984832 Generator output: 0.0073457574 Loss: 0.025419286 Generator output: 0.0026790632 Loss: 0.026378132 Generator output: 0.0026014717 Loss: 0.031021483 Generator output: 0.0037481724 Loss: 0.05135456 Generator output: 0.0057523246 Loss: 0.14441356 Generator output: 0.014102172 Loss: 0.059984554 Generator output: 0.0058901818 Loss: 0.053461727 Generator output: 0.0066189137 . It works in this example, we can see that loss is decreasing and generator output approaches 0. However, this works in this specific scenario, we can use euclidean distance when we measure the distance between 1D points, but how about images? Hence, for a general solution we will need something that works universally. And that is what Wasserstein GAN paper is about. . For the sake of this post, we will skip the details of how and why WGAN works (there are a few brilliant posts about that, please checkout out links in &quot;Further reading&quot; section below), but to put it simply, we can use the distance between the outputs of the discriminator as proxy for distances between distributions. (For this to work, we have to enforce a Lipschitz constraint which we will skip in this post, but will talk about how to enforce it in the next one.) . Let&#39;s try it out. . We measure the distance between the outputs of discriminator by simply subtracting one from another. . def d_loss_wgan(d_score_z, d_score_r): return tf.reduce_mean(d_score_z - d_score_r) . discriminator_wgan, d_optimizer = define_discriminator(act=None) train_discriminator(discriminator_wgan, d_optimizer, d_loss_wgan) . Total loss: -0.20631 Scores for real: 0.0 Scores for generated: -0.20631 Total loss: -92241.375 Scores for real: -128.61824 Scores for generated: -92370.0 Total loss: -1122561.8 Scores for real: -766.5613 Scores for generated: -1123328.2 Total loss: -4381084.5 Scores for real: -1648.9462 Scores for generated: -4382733.5 Total loss: -11101716.0 Scores for real: -2930.0134 Scores for generated: -11104646.0 Total loss: -20111968.0 Scores for real: -2881.5947 Scores for generated: -20114848.0 Total loss: -37373372.0 Scores for real: -2529.8655 Scores for generated: -37375904.0 Total loss: -62526256.0 Scores for real: -10314.391 Scores for generated: -62536570.0 Total loss: -69934790.0 Scores for real: -9289.29 Scores for generated: -69944080.0 Total loss: -135784350.0 Scores for real: -2764.6501 Scores for generated: -135787120.0 . Looks like it is working, loss is decreasing (although it is quite unusual to see negative loss). . So now we have discriminator which gives high scores for real examples. So the objective for generator would be to get as high scores as possible, hence we need just -discriminator(generated) . def g_loss_wgan(x): return tf.reduce_mean(-discriminator_wgan(x)) train_generator(g_loss_wgan) . Loss: 86786170.0 Generator output: 0.3962338 Loss: 2060925.2 Generator output: 0.009125391 Loss: 2406305.5 Generator output: 0.011163255 Loss: 816975.6 Generator output: 0.0036342242 Loss: 1002175.4 Generator output: 0.004520608 Loss: 1894261.4 Generator output: 0.008713374 Loss: 2398592.2 Generator output: 0.011115221 Loss: 1497964.8 Generator output: 0.007058435 Loss: 2221005.2 Generator output: 0.010142955 Loss: 2540864.5 Generator output: 0.011403117 . Code above actually is in line with WGAN (except, as I mentioned before, we are missing a method to enforce Lipschitz constraint). Three main changes are: . we removed sigmoid from the last layer of discriminator because we are not doing classification anymore. | we change the loss of discriminator to be the distance between real and generated outputs of discriminator (subtraction will do just fine for measuring distance between 1D points) | we change the generator loss to be just opposite to discriminator loss for generated input. | . We can see that this algorithm converges and the generated distribution approaches 0. This is all nice and good, however, as you can see, discriminator loss is actually a negative number, hence practitioners more often use some sort of not saturating loss. For example, hinge loss which is defined as follows: . def d_loss_hinge(d_score_z, d_score_r): d_loss_real = tf.nn.relu(1.0 - d_score_r) # Note: this changed d_loss_fake = tf.nn.relu(1.0 + d_score_z) # Note: this changed return tf.reduce_mean(d_loss_real) + tf.reduce_mean(d_loss_fake) . discriminator_hinge, d_optimizer = define_discriminator(act=None) train_discriminator(discriminator_hinge, d_optimizer, d_loss_hinge) . Total loss: 2.2130427 Scores for real: 0.0 Scores for generated: 0.21304269 Total loss: 0.046959918 Scores for real: 1.5518551 Scores for generated: -64.95206 Total loss: 0.0864621 Scores for real: 1.9289998 Scores for generated: -83.80411 Total loss: 0.035902765 Scores for real: 2.3413584 Scores for generated: -120.67704 Total loss: 0.06848332 Scores for real: 1.2159318 Scores for generated: -131.21864 Total loss: 0.07119481 Scores for real: 1.7761744 Scores for generated: -161.72647 Total loss: 0.041249227 Scores for real: 1.805813 Scores for generated: -213.31772 Total loss: 0.0 Scores for real: 1.5168437 Scores for generated: -301.31628 Total loss: 0.11920573 Scores for real: 1.8994367 Scores for generated: -270.10626 Total loss: 0.0 Scores for real: 1.4863261 Scores for generated: -312.72998 . The same non saturaing principle can be applied to the generator . def g_loss_hinge(x): return tf.reduce_mean(tf.nn.relu(1.0-discriminator_hinge(x))) . train_generator(g_loss_hinge) . Loss: 251.17415 Generator output: 0.50602597 Loss: 0.0 Generator output: 0.009517897 Loss: 0.0 Generator output: 0.008805303 Loss: 0.0 Generator output: 0.008805184 Loss: 0.0 Generator output: 0.008802602 Loss: 0.0 Generator output: 0.008800652 Loss: 0.0 Generator output: 0.008805037 Loss: 0.0 Generator output: 0.00880926 Loss: 0.0 Generator output: 0.008802065 Loss: 0.0 Generator output: 0.008806262 . That looks good, we are nearly done, but as I mentioned, we skipped &quot;Lipschitz constraint&quot; piece from Wasserstein GAN paper. We will focus on this in the next post. . Summary . I hope now you intuitively understand why we moved from the original GAN to the idea of having the discriminator that measures distance rather than classifying the input. Although the code changes are quite simple, they did improve the stability of the training significantly. And this advancement of GANs played an important role to allow us to generate pictures like that today. . . Further Reading . Read-through: Wasserstein GAN | From GAN to WGAN | GAN — Wasserstein GAN &amp; WGAN-GP | .",
            "url": "https://donatasrep.github.io/donatas.repecka/2020/04/05/gan-wgan.html",
            "relUrl": "/2020/04/05/gan-wgan.html",
            "date": " • Apr 5, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am CTO co-founder of Biomatter Designs. Currently working on developing deep learning based tools for various aspects of protein engineering. . LinkedIn .",
          "url": "https://donatasrep.github.io/donatas.repecka/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://donatasrep.github.io/donatas.repecka/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}