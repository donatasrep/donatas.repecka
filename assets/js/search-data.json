{
  
    
        "post0": {
            "title": "Performance comparison of the number of effective sequences algorithm",
            "content": "Performance comparison of the number of effective sequences algorithm . Introduction . From time to time I hear something along the lines ‘we should rewrite this in C to improve performance’. I always was sceptical about this idea, but I have not had any practical experience to be able to give a concrete example. During the process of making this post I explored the performance difference of one algorithm written in different ways including C and C++. The main questions I was intrigued about were: . How much faster is C/C++ in practise if I were to rewrite code? | Does the gain justify the effort? | Are any of the python performance libraries like Cypthon or Numba useful in practise? | . Note: a one example is not enough to draw any solid conclusions, but when it resonates with what you heard before that is at least reassuring. . I hope this will be yet another performance comparison which in combination with all other ones will be helpful to make a decision in your particular case. . Algorithm to be tested . The algorithm used to evaluate performance is the number of effective sequences (Nf) in the MSA. The algorithm is quite simple: the input is a list of equal length sequences, we need to calculate how many effectively unique sequences there are (a single number). In order to do so, firstly we need to calculate pairwise identities (what percentage of sequence is matching) and if the identity is higher than chosen threshold, we cluster them together. The inverse of cluster size gives you a weight of the sequence. A sum of weights is the Nf. Intuitively, if all sequences are similar, you will get one cluster, hence the Nf will be 1, at the other end of the spectrum, if all sequences are completely different, cluster sizes will be 1, hence the Nf will be equal to the number of sequences. We have the normalization term at the end to normalize by sequence length. . Pseudo code looks like this: . for seq1 in seqs: for seq2 in seqs: if count_mathes(seq1, seq2) &gt; threshold: weight +=1 meff += 1/weight meff = meff/(len(seq1)^0.5) . Code complexity is O(n^2) which means it does not scale well (later we will see how to make it O(n log(n))). . Setup . To perform testing I used the MSA which consists of 10000 sequences of length 683. The hardware used was: Intel(R) Core(TM) i7-10750H CPU @ 2.60GHz 16GB RAM + Nvidia 1650GTX. . Note: the results might vary wildly depending of the hardware you have. It is good to keep that in mind when analysing results. . Baselines . For baseline I used C and C++ implementations. As I do not consider myself an expert in C or C++ I borrowed existing implementations from existing sources: . C++ implementation was extracted from DeepMSA package. No changes have been made to it. | C implementation wast taken from Prody. Once again the code was run as it is. | . Results . In this section I display the results I got on my hardware. The implementations are extracted to separate notebooks for making this post bearable. While code might not be optimal, I did spend a significant amout of time trying to optimise the code. . Single threaded . Code Threads Time . C++ | 1 | 02:26.86 | . C | 1 | 00:41.35 | . Pure Python | 1 | ~7 hours* | . Numpy | 1 | 00:32.51 | . Cypthon | 1 | 00:32.43 | . Numba | 1 | 00:26.63 | . Julia | 1 | 04:51.00** | . * Pure python times were estimated on the timings I got on 100 sequences. . ** I am quite sure that Julia can be faster and the developer is to blame for its poor performance. . The very first thing, python is slow, very slow and if I would just compare Python vs C/C++ that would be the end of the story. However, the main strength of Python is its ecosystem. There are a bunch of optimised libraries you can use (just to be clear, I have not tested all of them, only the ones I thought to be good to test) which improves performance drastically. In my case, numpy, Cpython and numba implementations were faster than the C/C++, numba being the fastest one (I have opted to use Fastmath option which sacrifices precision for speed, more about this in the notebook. In general, it seems that any optimized library could achieve C/C++ like performance. . Multi threaded . While you could try to optimise the performance of the algorithm till perfection, in practise relative optimized code that scales will be way faster than optimal one on single thread. You can see that in the table below. . Core Threads Time . Numpy | 12 | 00:07.11 | . Numba | 12 | 00:05.12 | . Tensroflow | 12 | 00:39.88 | . Pytorch | 12 | 00:39.14 | . Some notes: C and C++ implementation did not have support multi threaded execution. Numpy version utilised the multiprocessing library to do multithreading, while numba has in-built support for that. I skipped Julia and Cypthon due to time constraints. I have also introduced Tensorflow and Pytorch as my work is around developing deep learning models and I try to compare those frameworks whenever I have a chance. While numba and numpy performance was as expected - much faster on multiple threads (just to note: there is some overhead due to multithreading, but it is still worth scaling up), the Tensorflow and Pytorch were both in comparison quite slow (I have to admit, not quite sure why). . With accelerator (GPU) . Nowadays, if you really want to go fast, you use accelerators such as GPU and TPU. While it requires to convert your code into the form that GPU likes (pretty much matrix multiplication), the gains sometimes are incredible. Even running on a mediocre GPU (Nvidia 1650GTX), I managed to have the best performance. What is more, Jax performance seems like out of the different world, but I will cover the details of the implementations separately. . Core Threads Time . Tensorflow | 1 | 00:06.10 | . Pytorch | 1 | 00:08.05 | . JAX | 1 | 00:00.13 | . Conclusion . During this experiment I have not found anything that would be ground breaking or new. Nevertheless, I the findings are extremelly useful for myself. The main takeaways are: . Pure python is slow. | Optimised python libraries can give you approximately C/C++ performance. | Scaling to multi threads or using GPU will give you the best performance. | Yet another thing which is not visible in the tables and is often overlooked is the solution itself. I have written some pieces several times to get these results (improving performance substantially). | . And if you were to ask me whether it is worth going to C/C++ for the performance reasons, I would say: unless you really need to squeeze every split of the second, you can stay in python and be at least competitive with C/C++ performance, not to mention development/maintenance side of things. .",
            "url": "https://donatasrep.github.io/donatas.repecka/markdown/2021/04/27/Performance-comparison.html",
            "relUrl": "/markdown/2021/04/27/Performance-comparison.html",
            "date": " • Apr 27, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Hands-on Spectral Normalization",
            "content": "Spectral normalization is a widely used technique to stabilize and improve the training of Generative adversarial networks. In nutshell, this normalization technique allows the measurement of meaningful distance between real and generated examples using discriminator. This measure is then used to train both, the discriminator and the generator. . Introduction . In my previous post I have walked through changes proposed in WGAN paper skipping an import piece about Lipschitz constraint. In this post I will discuss the most effective (up to this day) technique to satisfy this constraint. . As we talked in previous post, work of the authors of WGAN paper proposed to use the distance between the outputs of the discriminator as a proxy for distances between distributions. To put it simply, we want to measure how far two distributions are from each other. In order to be able to use the discriminator output as a proxy, we need to bound discriminator to be Lipschitz constrained. . Spectral Normalization . Interestingly, the authors of WGAN paper turned to the AI scientific community for the best approach. And a year later the paper called Spectral Normalization for Generative Adversarial Networks came out with a proposed solution. In comparison to previous attempts, this solution was superior due to its efficiency. So let&#39;s dig into it. . According to the paper, the Lipschitz constraint of the layer can be satisfied by dividing the weights of the layer by the largest singular value of the same weights. A really nice explanation and proof can be found here. Mathematically it looks like this: . $$W=W/ sigma(W)$$ . where $ sigma(W)$ is the largest singular value of the weights $W$. . An important observation by the authors of the paper is that if we do it for all layers, we will have a network that satisfies Lipschitz constraint. . Sounds simple enough, let&#39;s try it out. . import numpy as np . Let&#39;s take a convolutional filter of kernel size 3x3, 256 input channels and 512 output channels. Note: we reshaped the kernel so that we could calculate singular values. . W = np.random.normal(size = [3,3, 256, 512]).reshape([-1,512]) W.shape . (2304, 512) . Let&#39;s use numpy library to find singular values. numpy.linalg.svd can do that for us. The second output of numpy.linalg.svd function returns all singular values, we just need a maximum of that. . %%timeit s = np.linalg.svd(W, full_matrices=True)[1].max() s . 1 loop, best of 3: 1.02 s per loop . That is quite simple, however, it takes 1 second to calculate those values. Let&#39;s assume that we have 30 layers in the network, that would mean extra 30 seconds for each training step. Well, that is not what we could call efficient. . from scipy.sparse.linalg import svds . There is an alternative. scipy.sparse.linalg.svds function returns only the number k of the largest singular values. . %%timeit s = svds(W, k=1)[1] s . 10 loops, best of 3: 72 ms per loop . This is much faster: ~74ms, however for 30 layers we will slow down the training process by 2 seconds per step. Still not ideal. . Luckly, The authors of the paper proposed an alternative solution to get the largest singular value. This technique is known as power iteration. This looks like this: $$ v = W^ intercal u / ||W^ intercal u||_2$$ . $$ u = W v / ||W v||_2$$ . $$ W = W / u^ intercal W v$$ . Let&#39;s see if we can implement it. . $u$ needs to be sampled from an isotropic distribution at the beginning and it&#39;s dimensions should match the dimensions of the number of output channels. . u = np.random.normal(scale=0.2, size=[512]) u.shape . (512,) . First row of the equation . Note: we swapped u and W just for the sake of simplicity when implementing in numpy . v = u@W.T/np.linalg.norm(u@W.T, 2) v.shape . (2304,) . Second row of the equation . u = v@W / np.linalg.norm(v@W, 2) u.shape . (512,) . Third row of the equation . sigma = v@W@u.T sigma . 53.890708570486424 . Let&#39;s put everything together . def get_largets_singular_value(u): v = u@W.T/np.linalg.norm(u@W.T, 2) u = v@W / np.linalg.norm(v@W, 2) sigma = v@W@u.T return sigma, u . %%timeit sigma, _ = get_largets_singular_value(u.copy()) . 100 loops, best of 3: 2.06 ms per loop . print(&quot;Answer from numpy:&quot;, svds(W, k=1)[1].squeeze()) print(&quot;Power iteration outcome&quot;, sigma) . Answer from numpy: 70.21844081626173 Power iteration outcome 69.4230148155534 . Well, it is fast, ~40x faster, but it is not accurate. Well, that is expected because power iteration only approximates the largest singular value. The more iterations you perform, the more accurate approximation you would get. . Let’s see if that is true. . u_copy = u.copy() actual = svds(W, k=1)[1].squeeze() for i in range(20): sigma,u_copy = get_largets_singular_value(u_copy) print(&quot;Power iteration estimate: {:.2f} (actual: {:.2f})&quot;.format(sigma, actual)) . Power iteration estimate: 60.51 (actual: 70.22) Power iteration estimate: 63.85 (actual: 70.22) Power iteration estimate: 65.64 (actual: 70.22) Power iteration estimate: 66.69 (actual: 70.22) Power iteration estimate: 67.36 (actual: 70.22) Power iteration estimate: 67.82 (actual: 70.22) Power iteration estimate: 68.15 (actual: 70.22) Power iteration estimate: 68.40 (actual: 70.22) Power iteration estimate: 68.59 (actual: 70.22) Power iteration estimate: 68.74 (actual: 70.22) Power iteration estimate: 68.86 (actual: 70.22) Power iteration estimate: 68.97 (actual: 70.22) Power iteration estimate: 69.06 (actual: 70.22) Power iteration estimate: 69.13 (actual: 70.22) Power iteration estimate: 69.19 (actual: 70.22) Power iteration estimate: 69.25 (actual: 70.22) Power iteration estimate: 69.30 (actual: 70.22) Power iteration estimate: 69.35 (actual: 70.22) Power iteration estimate: 69.39 (actual: 70.22) Power iteration estimate: 69.42 (actual: 70.22) . This is indeed the case. However, doing power iteration 20 times gets us close to scipy performance. Fortunately, Spectral Normalization paper showed that you can do one or a small number of iterations per training step to get a good estimate throughout the entire training (doing incremental work every step). Hence, even using naive numpy implementation we can get to only extra 60ms (1.7ms*30) for each training iteration (assuming 30 layer network). . That does not sound too bad. . Note: keep in mind that these performance numbers are relative, they will depend on hardware. . We can try to port this to tensorflow and measure its performance. . import tensorflow as tf tf.config.list_physical_devices(&#39;GPU&#39;) . [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . @tf.function def get_largets_singular_value_tf(u, w): _v = tf.matmul(u, w, transpose_b=True) _v = tf.math.l2_normalize(_v) _u_m = tf.matmul(_v, w) _u = tf.math.l2_normalize(_u_m) sigma = tf.matmul(_u_m, _u, transpose_b=True) return sigma, _u . u_tf = tf.Variable(tf.random.normal(stddev=0.2, shape=[1,512]), trainable=False) w_tf = tf.Variable(tf.random.normal(shape = [3*3*256, 512]), trainable=False) . %%timeit sigma_tf, _ = get_largets_singular_value_tf(u_tf, w_tf) . The slowest run took 65.11 times longer than the fastest. This could mean that an intermediate result is being cached. 1000 loops, best of 3: 509 µs per loop . for i in range(20): sigma_tf, u_tf = get_largets_singular_value_tf(u_tf, w_tf) print(sigma_tf) . tf.Tensor([[69.43079]], shape=(1, 1), dtype=float32) . That is even better. 509µs with tensorflow implementation and executed on GPU (which usually is the case when you are training GANs). So if we assume 30 layers of similar size and one power iteration per training step, we only need ~15ms extra for the step. That is the reason why this technique is widely and successfully adopted. . In practice . Since we have all the pieces, we can create a layer wrapper to wrap any layer and perform power iteration on each feed forward pass. I have extended the code of here I have incorporated suggestions from the comments: . Turned off power iterations during the inherence. This makes sure that weights are not being changed doing inherence. | Used assigned operation instead of =. This speeds up the algorithm quite significantly (roughly by 35%). | . Additionally I have also added a hack to support mixed precision as well as the logic to support embedding layers. The final solution looks like this: . class SpectralNormalizationV2(tf.keras.layers.Wrapper): &quot;&quot;&quot; Attributes: layer: tensorflow keras layers (with kernel or embedding attribute) &quot;&quot;&quot; def __init__(self, layer, eps=1e-12, **kwargs): super(SpectralNormalizationV2, self).__init__(layer, name=layer.name + &quot;_sn&quot;, **kwargs) self.eps = eps self.is_embedding = isinstance(self.layer, tf.keras.layers.Embedding) def get_kernel_variable(self, attr=&#39;kernel&#39;): if not hasattr(self.layer, attr): raise ValueError(&#39;`SpectralNormalization` must wrap a layer that contains a `{}` for weights&#39;.format(attr)) return getattr(self.layer, attr) def build(self, input_shape=None): if not self.built: super(SpectralNormalizationV2, self).build(input_shape) if self.is_embedding: self.w = self.get_kernel_variable(&quot;embeddings&quot;) else: self.w = self.get_kernel_variable() self.autocast = hasattr(self.w, &quot;_variable&quot;) self.last_dim = self.w.shape[-1] self.u = self.add_weight(shape=[1, self.last_dim], initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02), name=&#39;sn_u&#39;, trainable=False, experimental_autocast=False) @tf.function def call(self, inputs, training=True): # Recompute weights for each training forward pass if training: self._compute_weights() output = self.layer(inputs, training=training) return output def _compute_weights(self): &quot;&quot;&quot;Generate normalized weights. This method will update the value of self.layer.kernel with the normalized value, so that the layer is ready for call(). &quot;&quot;&quot; if self.autocast: w = self.w._variable else: w =self.w w_reshaped = tf.reshape(w, [-1, self.last_dim]) _v = tf.matmul(self.u, w_reshaped, transpose_b=True) _v = tf.math.l2_normalize(_v, epsilon=self.eps) _u_m = tf.matmul(_v, w_reshaped) _u = tf.math.l2_normalize(_u_m, epsilon=self.eps) sigma = tf.matmul(_u_m, _u, transpose_b=True) self.u.assign(_u) self.w.assign(w / sigma) def compute_output_shape(self, input_shape): return self.layer.compute_output_shape(input_shape) . And the way you use is: . dense_sn = SpectralNormalizationV2(tf.keras.layers.Dense(units=100)) conv_sn = SpectralNormalizationV2(tf.keras.layers.Conv2D(filters=256, kernel_size=3)) emb_sn = SpectralNormalizationV2(tf.keras.layers.Embedding(input_dim=20, output_dim=100)) . I have also checked the performance in comparison to Pytorch implementation . Setup: . Batch size: 64 | Kernel of size :[16, 16, 256, 512] | Steps: 1000 | One power iteration per step | Hardware: NVIDIA V100 | . Results are: . Elapse time of Tensorflow SpectralNormalizationV2: 1.1081128120422363s | Elapse time of Pytorch official SpectralNorm implementation: 1.0783729553222656s | . Pytorch seems to be a little bit faster. I am not entirely sure whether it has to do with implementations of spectral normalization or just to the differences between Pytorch and Tensorflow. Regardless, those extra 30ms every 1000 steps should not be a game changer. . Summary . Spectral normalization is quite widely used in various implementations of GANs. One of the most famous applications is Biggan. All images below are generated by BigGAN architecture that uses Spectral normalization. This should highlight the importance of this advancement. . Throughout this post we focused on the implementation of power iteration proposed in Spectral Normalization for Generative Adversarial Networks paper. We have compared various methods to calculate the largest singular value and observed that the proposed power iteration method is the fastest and after multiple iterations it achieves reasonable accuracy. . .",
            "url": "https://donatasrep.github.io/donatas.repecka/2020/04/08/spectral-norm.html",
            "relUrl": "/2020/04/08/spectral-norm.html",
            "date": " • Apr 8, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Why we use WGAN?",
            "content": "Introduction . The original version of GAN was released in 2014. It was quite revolutionary and sparked the new line of research that is still very active. What is so revolutionary about GANs? The authors of the paper proposed to train two neural networks (discriminator and generator) stacked against each other in order to overcome the problem of having to create a loss function for the generator. . Now it seems rather obvious, however, at the time it was a wild idea. And even if it looked mathematically feasible, there was no indication that it could work in practise. Nevertheless, Ian Goodfellow and his colleagues demonstrated that such architecture can actually work even with resources at the time. See an examples from the same paper below (Note: images in yellow are the closest exmamples from training dataset) . Source: https://arxiv.org/pdf/1406.2661.pdf . In nutshell, generative adversarial networks consist of two networks: generator which generates data points and discriminator that evaluates whether input is coming from training dataset or is generated by generator. It is simple enough, however, it is quite important how the task of discriminator is formulated. That will be the main theme of this post. . Original discriminator objective . In the original version of GAN measure of real and generated examples was formulated as binary classification: is it real or is it generated. Let&#39;s look at the simple case, where we have true distribution (in other words real examples) that consists of 0&#39;s. A perfect discriminator will return 1 when input to the discriminator is 0 and output of the discriminator would be 0 otherwise. Despite the fact that discriminator is doing a great job at discriminating between real and generated, however, the generator does not receive meaningful direction for improving. For example, generator output of 0.1 and 1 will be scored the same, even though 0.1 is much closer to real distribution. Let&#39;s see this in action. . import tensorflow as tf import numpy as np from matplotlib import pyplot as plt np.set_printoptions(suppress=False, precision=5) . Discriminator . def d_loss(d_score_z, d_score_r): batch_size = d_score_z.shape[0] d_loss_z =tf.keras.losses.binary_crossentropy(tf.zeros(shape = [batch_size,1], dtype=tf.float32), d_score_z) d_loss_r =tf.keras.losses.binary_crossentropy(tf.ones(shape = [batch_size,1], dtype=tf.float32), d_score_r) return tf.reduce_mean(d_loss_z + d_loss_r) def define_discriminator(act=&quot;sigmoid&quot;): d_optimizer = tf.keras.optimizers.Adam(learning_rate=0.05) # Discriminator discriminator = tf.keras.models.Sequential() discriminator.add(tf.keras.layers.Dense(32, activation=&quot;relu&quot;)) discriminator.add(tf.keras.layers.Dense(32, activation=&quot;relu&quot;)) discriminator.add(tf.keras.layers.Dense(1, activation=act)) return discriminator, d_optimizer . def train_discriminator(discriminator, d_optimizer, d_loss, batch_size = 64): real_data = tf.zeros([batch_size, 1]) # Train discriminator till perfection for i in range(1000): with tf.GradientTape() as tape: z = tf.random.normal(shape=[batch_size, 1]) d_score_z, d_score_r = discriminator(z), discriminator(real_data) total_loss = d_loss(d_score_z, d_score_r) if i % 100 == 0: print(&quot;Total loss:&quot;, total_loss.numpy(), &quot;Scores for real:&quot;, d_score_r.numpy().mean(), &quot;Scores for generated:&quot;, d_score_z.numpy().mean()) grads = tape.gradient(total_loss, discriminator.trainable_variables) d_optimizer.apply_gradients(zip(grads, discriminator.trainable_variables)) . Disclaimer: we are creating an artificial situation just to demonstrate the underlying issue with using binary classification as a loss for discriminator. In reality, we would not train discriminators in this fashion. . discriminator, d_optimizer = define_discriminator() train_discriminator(discriminator, d_optimizer, d_loss) . Total loss: 1.2753901 Scores for real: 0.5 Scores for generated: 0.43882957 Total loss: 0.07295962 Scores for real: 0.9781779 Scores for generated: 0.015023634 Total loss: 0.014039941 Scores for real: 0.986065 Scores for generated: 7.0891906e-06 Total loss: 0.021772312 Scores for real: 0.9784628 Scores for generated: 5.510665e-11 Total loss: 0.075927064 Scores for real: 0.9949504 Scores for generated: 0.015457453 Total loss: 0.11446469 Scores for real: 0.9977589 Scores for generated: 0.038589276 Total loss: 0.00037504316 Scores for real: 0.99962485 Scores for generated: 4.787702e-14 Total loss: 0.07557466 Scores for real: 0.9988571 Scores for generated: 0.017265305 Total loss: 0.0038048518 Scores for real: 0.9962043 Scores for generated: 2.0079315e-06 Total loss: 0.0021338563 Scores for real: 0.9978684 Scores for generated: 6.928249e-14 . So we have got a nearly perfect discriminator that gives 1 for real distribution and 0 for anything else. Let&#39;s train the generator using such discriminator. . Generator . Here we will try to train the generator using perfect discriminator. Again, you would not do that in practise. . def g_loss(x, discriminator=discriminator): return tf.reduce_mean(1.0-discriminator(x)) def train_generator(g_loss, batch_size=64): g_optimizer = tf.keras.optimizers.Adam(learning_rate=0.05) generator = tf.keras.layers.Dense(1, activation=&#39;tanh&#39;) for i in range(1000): with tf.GradientTape() as tape: z = tf.random.normal(shape=[batch_size, 1]) gen = generator(z) loss = g_loss(gen) if i % 100 == 0: print(&quot;Loss:&quot;, loss.numpy(), &quot;Generator output: &quot;,tf.reduce_mean(tf.abs(gen)).numpy()) grads = tape.gradient(loss, generator.trainable_variables) g_optimizer.apply_gradients(zip(grads, generator.trainable_variables)) t = plt.scatter(np.zeros(z.shape), z.numpy(), label=&quot;True&quot;) g = plt.scatter(gen.numpy(), z.numpy(), alpha=0.5) plt.xlim(-1, 1) plt.legend((t,g),(&quot;True distribution&quot;, &quot;Generated distribution&quot;)) . train_generator(g_loss) . Loss: 0.98446023 Generator output: 0.5757177 Loss: 1.0 Generator output: 0.62681496 Loss: 0.99996835 Generator output: 0.68048704 Loss: 1.0 Generator output: 0.6950605 Loss: 1.0 Generator output: 0.7098987 Loss: 1.0 Generator output: 0.7288726 Loss: 1.0 Generator output: 0.9237368 Loss: 1.0 Generator output: 0.9254451 Loss: 1.0 Generator output: 0.922577 Loss: 1.0 Generator output: 0.923764 . It does not converge, the generator does not learn to produce 0&#39;s and loss does not decrease. We could find a way to train so that it converges, however, ideally we would want something that would be able to converge in this situation as well. . This issue was described in famous Wasserstein GAN paper. In this paper authors proposed to change binary classification to something that measures distance between generated outputs and real examples. . Let&#39;s try it out with our simplified example. . WGAN . A very naive approach would be to measure Euclidean distance between real and generated distributions. Let’s see if that helps. . def euclidean_distance(x): return tf.reduce_mean(tf.norm(x, &#39;euclidean&#39;)) train_generator(euclidean_distance) . Loss: 1.8566905 Generator output: 0.19034971 Loss: 0.06090928 Generator output: 0.005873415 Loss: 0.05984832 Generator output: 0.0073457574 Loss: 0.025419286 Generator output: 0.0026790632 Loss: 0.026378132 Generator output: 0.0026014717 Loss: 0.031021483 Generator output: 0.0037481724 Loss: 0.05135456 Generator output: 0.0057523246 Loss: 0.14441356 Generator output: 0.014102172 Loss: 0.059984554 Generator output: 0.0058901818 Loss: 0.053461727 Generator output: 0.0066189137 . It works in this example, we can see that loss is decreasing and generator output approaches 0. However, this works in this specific scenario, we can use euclidean distance when we measure the distance between 1D points, but how about images? Hence, for a general solution we will need something that works universally. And that is what Wasserstein GAN paper is about. . For the sake of this post, we will skip the details of how and why WGAN works (there are a few brilliant posts about that, please checkout out links in &quot;Further reading&quot; section below), but to put it simply, we can use the distance between the outputs of the discriminator as proxy for distances between distributions. (For this to work, we have to enforce a Lipschitz constraint which we will skip in this post, but will talk about how to enforce it in the next one.) . Let&#39;s try it out. . We measure the distance between the outputs of discriminator by simply subtracting one from another. . def d_loss_wgan(d_score_z, d_score_r): return tf.reduce_mean(d_score_z - d_score_r) . discriminator_wgan, d_optimizer = define_discriminator(act=None) train_discriminator(discriminator_wgan, d_optimizer, d_loss_wgan) . Total loss: -0.20631 Scores for real: 0.0 Scores for generated: -0.20631 Total loss: -92241.375 Scores for real: -128.61824 Scores for generated: -92370.0 Total loss: -1122561.8 Scores for real: -766.5613 Scores for generated: -1123328.2 Total loss: -4381084.5 Scores for real: -1648.9462 Scores for generated: -4382733.5 Total loss: -11101716.0 Scores for real: -2930.0134 Scores for generated: -11104646.0 Total loss: -20111968.0 Scores for real: -2881.5947 Scores for generated: -20114848.0 Total loss: -37373372.0 Scores for real: -2529.8655 Scores for generated: -37375904.0 Total loss: -62526256.0 Scores for real: -10314.391 Scores for generated: -62536570.0 Total loss: -69934790.0 Scores for real: -9289.29 Scores for generated: -69944080.0 Total loss: -135784350.0 Scores for real: -2764.6501 Scores for generated: -135787120.0 . Looks like it is working, loss is decreasing (although it is quite unusual to see negative loss). . So now we have discriminator which gives high scores for real examples. So the objective for generator would be to get as high scores as possible, hence we need just -discriminator(generated) . def g_loss_wgan(x): return tf.reduce_mean(-discriminator_wgan(x)) train_generator(g_loss_wgan) . Loss: 86786170.0 Generator output: 0.3962338 Loss: 2060925.2 Generator output: 0.009125391 Loss: 2406305.5 Generator output: 0.011163255 Loss: 816975.6 Generator output: 0.0036342242 Loss: 1002175.4 Generator output: 0.004520608 Loss: 1894261.4 Generator output: 0.008713374 Loss: 2398592.2 Generator output: 0.011115221 Loss: 1497964.8 Generator output: 0.007058435 Loss: 2221005.2 Generator output: 0.010142955 Loss: 2540864.5 Generator output: 0.011403117 . Code above actually is in line with WGAN (except, as I mentioned before, we are missing a method to enforce Lipschitz constraint). Three main changes are: . we removed sigmoid from the last layer of discriminator because we are not doing classification anymore. | we change the loss of discriminator to be the distance between real and generated outputs of discriminator (subtraction will do just fine for measuring distance between 1D points) | we change the generator loss to be just opposite to discriminator loss for generated input. | . We can see that this algorithm converges and the generated distribution approaches 0. This is all nice and good, however, as you can see, discriminator loss is actually a negative number, hence practitioners more often use some sort of not saturating loss. For example, hinge loss which is defined as follows: . def d_loss_hinge(d_score_z, d_score_r): d_loss_real = tf.nn.relu(1.0 - d_score_r) # Note: this changed d_loss_fake = tf.nn.relu(1.0 + d_score_z) # Note: this changed return tf.reduce_mean(d_loss_real) + tf.reduce_mean(d_loss_fake) . discriminator_hinge, d_optimizer = define_discriminator(act=None) train_discriminator(discriminator_hinge, d_optimizer, d_loss_hinge) . Total loss: 2.2130427 Scores for real: 0.0 Scores for generated: 0.21304269 Total loss: 0.046959918 Scores for real: 1.5518551 Scores for generated: -64.95206 Total loss: 0.0864621 Scores for real: 1.9289998 Scores for generated: -83.80411 Total loss: 0.035902765 Scores for real: 2.3413584 Scores for generated: -120.67704 Total loss: 0.06848332 Scores for real: 1.2159318 Scores for generated: -131.21864 Total loss: 0.07119481 Scores for real: 1.7761744 Scores for generated: -161.72647 Total loss: 0.041249227 Scores for real: 1.805813 Scores for generated: -213.31772 Total loss: 0.0 Scores for real: 1.5168437 Scores for generated: -301.31628 Total loss: 0.11920573 Scores for real: 1.8994367 Scores for generated: -270.10626 Total loss: 0.0 Scores for real: 1.4863261 Scores for generated: -312.72998 . The same non saturaing principle can be applied to the generator . def g_loss_hinge(x): return tf.reduce_mean(tf.nn.relu(1.0-discriminator_hinge(x))) . train_generator(g_loss_hinge) . Loss: 251.17415 Generator output: 0.50602597 Loss: 0.0 Generator output: 0.009517897 Loss: 0.0 Generator output: 0.008805303 Loss: 0.0 Generator output: 0.008805184 Loss: 0.0 Generator output: 0.008802602 Loss: 0.0 Generator output: 0.008800652 Loss: 0.0 Generator output: 0.008805037 Loss: 0.0 Generator output: 0.00880926 Loss: 0.0 Generator output: 0.008802065 Loss: 0.0 Generator output: 0.008806262 . That looks good, we are nearly done, but as I mentioned, we skipped &quot;Lipschitz constraint&quot; piece from Wasserstein GAN paper. We will focus on this in the next post. . Summary . I hope now you intuitively understand why we moved from the original GAN to the idea of having the discriminator that measures distance rather than classifying the input. Although the code changes are quite simple, they did improve the stability of the training significantly. And this advancement of GANs played an important role to allow us to generate pictures like that today. . . Further Reading . Read-through: Wasserstein GAN | From GAN to WGAN | GAN — Wasserstein GAN &amp; WGAN-GP | .",
            "url": "https://donatasrep.github.io/donatas.repecka/2020/04/05/gan-wgan.html",
            "relUrl": "/2020/04/05/gan-wgan.html",
            "date": " • Apr 5, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am CTO co-founder of Biomatter Designs. Currently working on developing deep learning based tools for various aspects of protein engineering. . LinkedIn .",
          "url": "https://donatasrep.github.io/donatas.repecka/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://donatasrep.github.io/donatas.repecka/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}