{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VrE1kciUIxD5"
   },
   "source": [
    "# Performance Analysis - CUDA\n",
    "> Number of effective sequences implemented in CUDA\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- author: Donatas Repečka\n",
    "- categories: [performance]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OVXv9X4d_dND"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "skMnA5NeIU4O"
   },
   "source": [
    "In [the previous post](https://donatasrep.github.io/donatas.repecka/performance/2021/04/27/Performance-comparison.html) I have compared various languages and libraries in terms of their speed. This notebook contains a version of CUDA that has CUDA kernels to perform the calculations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a test case, we will use calculation of effective sequences (Nf) in the MSA. Just to remind the pseudo code looks like this:\n",
    "\n",
    "```\n",
    "for seq1 in seqs:\n",
    "  for seq2 in seqs:\n",
    "    if count_mathes(seq1, seq2) > threshold:\n",
    "      weight +=1\n",
    "  meff += 1/weight\n",
    " \n",
    "meff = meff/(len(seq1)^0.5)\n",
    "```\n",
    "\n",
    "Just note, that algorithmic complexity is ```O(n*log(n))``` or ```O(n^2)``` depending on implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reminder of speeds for 2500 sequences of MSA length 683.\n",
    "\n",
    "| Library  | Time (ms)| Time (ms)| \n",
    "|----------|----------|----------|\n",
    "|   Numpy  | Singlethreaded | 2000 | \n",
    "|   Numba  | Multithreaded |   669    | \n",
    "|   Numpy  | Multithreaded |   503    |  \n",
    "| Pytorch  | GPU      | 389 |\n",
    "|   Numba  | Multithreaded + Low precision |   176    | \n",
    "|   Jax    |   GPU    |   116    |\n",
    "\n",
    "\\* [Why is JAX so fast?](https://github.com/google/jax/discussions/11078)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import  to note that a lot of of ideas were taken from Heremy Howard's lectures: \n",
    "* https://www.youtube.com/watch?v=nOxKexn3iBo\n",
    "* https://www.youtube.com/watch?v=eUuGdh3nBGo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget https://raw.githubusercontent.com/donatasrep/donatas.repecka/master/data/picked_msa.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install numpy\n",
    "# ! pip install pandas\n",
    "# ! pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# ! pip install  wurlitzer ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    fasta_df = pd.read_csv(path, lineterminator=\">\", header=None)\n",
    "    fasta_df[['id', 'seq']] = fasta_df[0].str.split('\\n', expand=True)[[0,1]]\n",
    "    return fasta_df.seq.to_numpy(dtype=str)\n",
    "seqs = get_data('picked_msa.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['QRVAMDMHLRHMQYKMPDNYRGYQWRYDFPTIWIMQGGEGRINLTIPRAAPRKIQYQPNYAKTSPEYTMILRGAKPHATLRRDTGARNPCTYPWVQPRRPNKKPGQVDSLPNTDRLMNVKLNMDAHPQNPVPSVRRYGRPPYHTWSQNRDRKDIFFCRLRHPQYTPRFIFMWLDPMEGAYGTRQWQLRPTGFKVYPQRFDMIRHLDRIDFLATHPDRARRTSNDDRRRIIQYMRDCTTGQLIRRTQRGVRGSLIYIKKITACTKTNKSRGSRRKMGTLWRQMILKMQNEGTPQPKQPLTRLMFRGQIGFCLRWAPGFIMEIIPRSPRQKQGQRFVWGPDTTKISTMAKTIRRQIPIRGRPDRQKPFMRFNDMKTGSIKGNRQTKRGPWLLPNISRYKKRDPNIIGWPPCRLENIIAINRQKDIGKRQEGTPRNARIPHINMIRNKTPPDLWRYVILEQHSTIRRRDQDLNTFGPNNIRVPIQRRPRPMMGRTHVCRDARIMIHGQLPDRQDTPRPDIDHQWSQDGYYRRTMGTLKIGIINQQMQMNAWIKIRPIGQYITLIMRAIRQTIRKEIMGKKRVNSQDYLSIFIQIHAHGMASLLLGRRQYMGKMAKVKDMARIRRKIRFRPKLRYAAHCPGHTKGHYIMILGVLEIDTIILKIIAQMQPDQYQRRAMVYYPVYLRAV',\n",
       "       'QRVRMDMSLRSMQYIMPMIYPGYQWRYDFPTIWIMQGGEGRINLTIPRAAPRKIQYQPNYAKTSPEYTMILRGAKPHATLRRDTGARNPCTYPWVQPRRPNKKPGQVDSLPNTDRLMNVKLNMDAHPQNPVPSVRRYGRPPYHLWSQNRDRKDIFFCRLRHPQYTPRFIFMWLDPGEGAYGTRQWQLRPTGFKVYPQRFDMIRHLDRIDFLATHPDRARRTSNDDARRIIQYMRDCTTGQLIRRRQRGVRGSLIYIKKITRCTKTNKSRGSRRKMGTLWRQMILKMQNEGTPQPKQPLTRLMFRGQIGFCLRWAPGFIMEIIPRSPRQKQGQRFVWGPDTTKISTMAKTIRRQIPIRGRPDRQKPFMRFNDMKTGSIKGNRQTKRGPWLLPNISRYKKRDPNIIGWPPCRLENIIAINRQKDIGKRQEGTPRNARIPHINMIRNKTPPDLWRYVILEQHSTIERRDQDLNTFGPNNIRVPIQRRPRPMMGIPHDCRMARIMQHGQLPDRQDTPAPDIDHQWSQDGYYFRTMGTLKIGIINQQMQMNAWILIRPIGQYITLIMRAIRQTIRKEIMGKLRVNSQDYLSIFIQIHAHGMASLLLGRRQYMGLMAKVKYMARIRRKIRFRPKLRYAAHCRGHTKGDYIMILGVLRIDTIILMIIAKQPPDQYQRRAMDYQPS-----'],\n",
       "      dtype='<U683')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each GPU consists of several Streaming Multiprocessors (SM). In case of RTX 3090 - we have of them 82.\n",
    "* Each SM contains CUDA cores. In case of RTX 3090 - we have 128 of them.\n",
    "* That adds up to 10496 CUDA cores which is roughly equal number of operations that can be run in parallel. \n",
    "\n",
    "Interestingly, A100 has only 6912 CUDA cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here, we will write CUDA simulator in python, it is slow, but makes it easier to develop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "dim3 = namedtuple('dim3', ['x','y','z'], defaults=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdiv(a,b):\n",
    "    \"Int ceiling division of `a` over `b`\"\n",
    "    return (a+b-1)//b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blk_kernel2d(f, blocks, threads, *args):\n",
    "    for i0 in range(blocks.y):\n",
    "        for i1 in range(blocks.x):\n",
    "            for j0 in range(threads.y):\n",
    "                for j1 in range(threads.x): f(dim3(i1,i0), dim3(j1,j0), threads, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nf(blockIdx, threadIdx, blockDim, seqs, out, n_seqs, seq_len):\n",
    "    id1 = blockIdx.y*blockDim.y + threadIdx.y\n",
    "    id2 = blockIdx.x*blockDim.x + threadIdx.x\n",
    "\n",
    "    if (id1>=n_seqs or id2>=n_seqs): return\n",
    " \n",
    "    seq1 = seqs[id1]\n",
    "    seq2 = seqs[id2]\n",
    "    identity = 0\n",
    "    \n",
    "    for p in range(seq_len):\n",
    "        identity = identity + int(seq1[p] == seq2[p])\n",
    "    identity = identity/seq_len\n",
    "\n",
    "    is_more = identity > 0.8\n",
    "\n",
    "    out[id1 * n_seqs + id2] = is_more\n",
    "    out[id2 * n_seqs + id1] = is_more\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nf_2d(seqs):\n",
    "    n_seqs  = seqs.shape[0]\n",
    "    seq_len = len(seqs[0])\n",
    "    output = torch.zeros((n_seqs, n_seqs), dtype=float)\n",
    "    tpb = dim3(16,16)\n",
    "    blocks = dim3(cdiv(n_seqs, tpb.x), cdiv(n_seqs, tpb.y))\n",
    "    blk_kernel2d(get_nf, blocks, tpb, seqs, output.flatten(), n_seqs, seq_len)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1801, dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_mask = nf_2d(seqs[:100])\n",
    "python_nf = (1/ python_mask.sum(-1)).sum() / (len(seqs[0])**0.5)\n",
    "python_nf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "797 ms ± 5.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "python_mask = nf_2d(seqs[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA\n",
    "\n",
    "Once we have python version, we need to translate this to CUDA/C++ code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']='0'\n",
    "from torch.utils.cpp_extension import load_inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext wurlitzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cuda(cuda_src, cpp_src, funcs, opt=True, verbose=False, name=None):\n",
    "    \"Simple wrapper for torch.utils.cpp_extension.load_inline\"\n",
    "    if name is None: name = funcs[0]\n",
    "    flags = \"-O3 -Xptxas -O3 -Xcompiler -O3\" if opt else \"-O0 -Xptxas -O0 -Xcompiler -O0\"\n",
    "    return load_inline(cuda_sources=[cuda_src], cpp_sources=[cpp_src], functions=funcs,\n",
    "                       extra_cuda_cflags=[flags], verbose=verbose, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_begin = r'''\n",
    "#include <torch/extension.h>\n",
    "#include <stdio.h>\n",
    "#include <c10/cuda/CUDAException.h>\n",
    "\n",
    "#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x \" must be a CUDA tensor\")\n",
    "#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n",
    "#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
    "#define CUDA_ERR(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
    "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)\n",
    "{\n",
    "   if (code != cudaSuccess) \n",
    "   {\n",
    "      fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
    "      if (abort) exit(code);\n",
    "   }\n",
    "}\n",
    "__host__ __device__ inline unsigned int cdiv(unsigned int a, unsigned int b) { return (a+b-1)/b;}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_src = cuda_begin + r'''\n",
    "\n",
    "__device__ float get_identity(uint8_t* seq1, uint8_t* seq2, int seq_len) {\n",
    "    float identity = 0.0f;\n",
    "    for (int p = 0; p < seq_len; ++p) {\n",
    "        identity += (seq1[p] == seq2[p]) ? 1 : 0;\n",
    "    }\n",
    "    return identity / seq_len;\n",
    "}\n",
    "\n",
    "__global__ void nf_kernel(uint8_t* seqs, uint8_t* out, int n_seqs, int seq_len) {\n",
    "    int id1 = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int id2 = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (id1 >= n_seqs || id2 >= n_seqs) return;\n",
    "\n",
    "    uint8_t* seq1 = &seqs[id1 * seq_len];\n",
    "    uint8_t* seq2 = &seqs[id2 * seq_len];\n",
    "\n",
    "    float identity = get_identity(seq1, seq2, seq_len);\n",
    "\n",
    "    bool is_more = identity > 0.8f;\n",
    "\n",
    "    out[id1 * n_seqs + id2] = is_more;\n",
    "    out[id2 * n_seqs + id1] = is_more;\n",
    "}\n",
    "\n",
    "torch::Tensor get_nf(torch::Tensor input) {\n",
    "    CHECK_INPUT(input);\n",
    "    int n_seqs = input.size(0);\n",
    "    int seq_len = input.size(1);\n",
    "    auto output = torch::empty({n_seqs,n_seqs}, input.options());\n",
    "    dim3 tpb(16,16);\n",
    "    dim3 blocks(cdiv(n_seqs,tpb.x), cdiv(n_seqs,tpb.y));\n",
    "    nf_kernel<<<blocks, tpb>>>(\n",
    "        input.data_ptr<uint8_t>(), output.data_ptr<uint8_t>(), n_seqs, seq_len);\n",
    "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
    "    return output;\n",
    "}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/drepecka/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/drepecka/.cache/torch_extensions/py311_cu121/get_nf/build.ninja...\n",
      "Building extension module get_nf...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module get_nf...\n"
     ]
    }
   ],
   "source": [
    "module = load_cuda(cuda_src, \"torch::Tensor get_nf(torch::Tensor input);\", ['get_nf'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nf_cuda(seqs):\n",
    "    seqs = torch.from_numpy(seqs.view(np.int32).reshape(seqs.shape[0], -1)).byte().contiguous().cuda()\n",
    "    cuda_mask = module.get_nf(seqs)\n",
    "    cuda_nf = (1/cuda_mask.sum(-1)).sum() / (len(seqs[0])**0.5)\n",
    "    return cuda_nf.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_nf = get_nf_cuda(seqs[:2500])\n",
    "np.testing.assert_almost_equal(cuda_nf, 19.919439, decimal=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115 ms ± 9.06 ms per loop (mean ± std. dev. of 3 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 3 -r 3\n",
    "get_nf_cuda(seqs[:2500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba with CUDA\n",
    "\n",
    "Numba also allows us to access CUDA and write kernels even without leaving python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['NUMBA_ENABLE_CUDASIM']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CUDA devices\n",
      "id 0    b'NVIDIA GeForce GTX 1650 Ti'                              [SUPPORTED]\n",
      "                      Compute Capability: 7.5\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 1\n",
      "                                    UUID: GPU-3f8cf4e0-90bd-e695-da8e-354107ab341b\n",
      "                                Watchdog: Enabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "Summary:\n",
      "\t1/1 devices are supported\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numba import cuda\n",
    "cuda.detect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "@cuda.jit\n",
    "def nf_kernel(seqs, out, seq_len, h):\n",
    "\n",
    "    id1 = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
    "    id2 = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "\n",
    "    if (id1 >= h or id2 >= h): return\n",
    "\n",
    "    seq1 = seqs[id1]\n",
    "    seq2 = seqs[id2]\n",
    "\n",
    "\n",
    "    identity = 0\n",
    "    for p in range(seq_len):\n",
    "        identity += (seq1[p] == seq2[p])\n",
    "    identity = identity / seq_len\n",
    "\n",
    "\n",
    "    is_more = identity > 0.8\n",
    "\n",
    "    out[id1, id2] = is_more\n",
    "    out[id2, id1] = is_more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nf_numba(seqs):\n",
    "    h = seqs.shape[0]\n",
    "    tpb = dim3(16,16)\n",
    "    blocks = dim3(cdiv(h,tpb.x), cdiv(h,tpb.y))\n",
    "    seqs_cuda_numba = cuda.to_device(seqs.view(np.int32).reshape(h, -1).astype(np.byte))\n",
    "    out = cuda.to_device(np.zeros((h, h), dtype=np.byte))\n",
    "    nf_kernel[blocks, tpb](seqs_cuda_numba, out, len(seqs[0]), h)\n",
    "    numba_nf = sum(1/out.copy_to_host().sum(1))/len(seqs[0])**0.5\n",
    "    return numba_nf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf_numba_cuda = get_nf_numba(seqs[:2500])\n",
    "np.testing.assert_almost_equal(nf_numba_cuda, 19.919439, decimal=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124 ms ± 7.06 ms per loop (mean ± std. dev. of 3 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 3 -r 3\n",
    "get_nf_numba(seqs[:2500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Numba is slower, it may be enough for a large number of pratical applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Now we have these speeds for 2500 sequences of MSA length 683.\n",
    "\n",
    "| Library  | Time (ms)| Time (ms)| \n",
    "|----------|----------|----------|\n",
    "|   Numpy  | Singlethreaded | 2000 | \n",
    "|   Numba  | Multithreaded |   669    | \n",
    "|   Numpy  | Multithreaded |   503    |  \n",
    "| Pytorch  | GPU      | 389 |\n",
    "|   Numba  | Multithreaded + Low precision |   176    | \n",
    "|   Jax    |   GPU    |   116    |\n",
    "|   CUDA    |   GPU    |   115    |\n",
    "|   Numba (CUDA)    |   GPU    |   124    |\n",
    "\n",
    "While this is may not be the most representative case, it still gives some intuition on potential speed improvements. Definitely, pure python is slow, however, I see python as a interface to other languages through various libraries (Numpy, Numba, Jax, Polars). By utilising these libraries, you benefit from well-tested and optimised code, which could take time to achieve when doing from scratch (for example, in C/Rust, etc). In most cases, with some thinking, the problem at hand can be converted to fit the framework of the library. If that does not work or if you really need to optimise every microsecond, then going the lowest level will be always (almost) fastest solution performance wise. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "2020-04-08-spectral-norm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
