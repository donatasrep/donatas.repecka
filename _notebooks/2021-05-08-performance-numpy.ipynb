{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VrE1kciUIxD5"
   },
   "source": [
    "# Performance Analysis -  Numpy\n",
    "> Number of effective sequences implemented in Numpy\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- author: Donatas Repečka\n",
    "- categories: [performance]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OVXv9X4d_dND"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "skMnA5NeIU4O"
   },
   "source": [
    "In [the previous post](https://donatasrep.github.io/donatas.repecka/performance/2021/04/27/Performance-comparison.html) I have compared various languages and libraries in terms of their speed. This notebook contains the code used in the comparison as well as some details about the choices made to improve the performance of numpy implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/donatasrep/donatas.repecka/blob/master/data/picked_msa.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install numpy\n",
    "# ! pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    fasta_df = pd.read_csv(path, lineterminator=\">\", header=None)\n",
    "    fasta_df[['id', 'seq']] = fasta_df[0].str.split('\\n', expand=True)[[0,1]]\n",
    "    return fasta_df.seq.to_numpy(dtype=str)\n",
    "seqs = get_data('picked_msa.fasta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to remind the pseudo code looks like this:\n",
    "\n",
    "```\n",
    "for seq1 in seqs:\n",
    "  for seq2 in seqs:\n",
    "    if count_mathes(seq1, seq2) > threshold:\n",
    "      weight +=1\n",
    "  meff += 1/weight\n",
    " \n",
    "meff = meff/(len(seq1)^0.5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Numpy implementation is based on pure python implementation which can be found [here](https://donatasrep.github.io/donatas.repecka/performance/2021/05/08/performance-pure-python.html). \n",
    "\n",
    "The main differences are:\n",
    "* uses numpy arrays and operators. \n",
    "* sequences are in vectorised fashion using `np.equal` rather than looping over each element in the sequence.\n",
    "* sequences are converted to arrays of integers rather than characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_nf_python_numpy(seqs, threshold=0.8):\n",
    "    seqs = seqs.view(np.uint32).reshape(seqs.shape[0], -1)\n",
    "    n_seqs = len(seqs)\n",
    "    is_same_cluster = np.ones([n_seqs, n_seqs], np.bool_) \n",
    "    for i in range(n_seqs):\n",
    "        for j in range(i+1, n_seqs):\n",
    "            identity = np.equal(seqs[i], seqs[j]).mean()\n",
    "            is_more = np.greater(identity, threshold)\n",
    "            is_same_cluster[i,j] = is_more\n",
    "            is_same_cluster[j,i] = is_more\n",
    "    meff = 1.0/np.sum(is_same_cluster,1)\n",
    "    return meff.sum()/(seqs.shape[-1]**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.3 s ± 181 ms per loop (mean ± std. dev. of 3 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 3 -r 3\n",
    "get_nf_python_numpy(seqs[:2500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18006706787628665"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nf_python_numpy(seqs[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although performance gain is substantial, I still have two loops which is one of the first things you want to tackle when trying to improve algorithm speed. In practise, you want to use vectorization to take advantage of CPU ability to perform the same operation on vectors. \n",
    "\n",
    "Ideally, I would like to do all to all pairwise comparison in one vectorized operation. Unfortunately, this is not a viable option due to memory constraints . Such operation will require `n * n * l` size matrix. In my case, that  would result into ~70GB size matrix (n = 10000, l = 683, boolean uses 1 byte)\n",
    "\n",
    "$$10000^2 *683 = 68.3*10^9 \\ bytes = 68.3 \\ gigabytes$$\n",
    "\n",
    "To be able to run such an algorithm on an everyday computer, the memory requirement has to be lower. Usually, this can be solved with batching, taking only a portion of data at the time. In this case, I will perform a one-to-all comparison at the time. This will require `n*l` memory which is 10000 (~7MB)  times less than for the all-to-all version. \n",
    "\n",
    "\n",
    "Note, I still can exploit the fact that the matrix of identity is symmetric which means that only the first iteration has to be done with `n-1` sequences, the second sequence needs only be compared with `n-2` and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_to_all_comparison(seqs, threshold=0.8):\n",
    "    pairwise_id = np.equal(seqs[1:], seqs[0].T).mean(1)\n",
    "    return pairwise_id > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nf_numpy_v2(seqs):\n",
    "    seqs = seqs.view(np.uint32).reshape(seqs.shape[0], -1)\n",
    "    n_seqs, seq_len = seqs.shape\n",
    "    is_same_cluster = np.ones([n_seqs, n_seqs],np.bool_)\n",
    "    for i in range(n_seqs-1):\n",
    "        out = get_one_to_all_comparison(seqs[i:])\n",
    "        is_same_cluster[i, i+1:] = out\n",
    "        is_same_cluster[i+1:, i] = out\n",
    "    s = 1.0/is_same_cluster.sum(1)\n",
    "    return s.sum()/(seq_len**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.99 s ± 47.7 ms per loop (mean ± std. dev. of 3 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 3 -r 3\n",
    "get_nf_numpy_v2(seqs[:2500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18006706787628665"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nf_numpy_v2(seqs[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using numpy, the algorithm's runtime decreases significantly. The gain on 1% of the data is roughly 100 times (might differ depending on the hardware). The improvement will scale with the amount of data that is being processed so on the data set  the gain will be even larger. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a limit to how much an algorithm can be improved. However, more speed can be gained by simply exploiting the sheer size of infrastructure, i.e. multiple CPUs/machines. Unfortunately, this rarely happens automatically, unless you use libraries that have this ability built-in. Although Numpy has some functionality that runs in parallel, in this case some additional work is required. \n",
    "\n",
    "In this case, sequence comparison does not need to be performed sequentially, thus can be parallelised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some effort were made to make code below work on Windows machines. As a result, we have to create a seperate file for worker code. More information [here](https://medium.com/@grvsinghal/speed-up-your-python-code-using-multiprocessing-on-windows-and-jupyter-or-ipython-2714b49d6fac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! wget https://github.com/donatasrep/donatas.repecka/blob/master/_notebooks/worker.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from worker import *\n",
    "\n",
    "def get_nf_numpy_mp(seqs, threads=None):\n",
    "    \n",
    "    seqs = seqs.view(np.uint32).reshape(seqs.shape[0], -1)\n",
    "    n_seqs, seq_len = seqs.shape   \n",
    "    \n",
    "    with multiprocessing.Pool(threads, initializer=init, initargs=[seqs]) as pool:\n",
    "        results = pool.map(get_one_to_all_comparison_global, range(n_seqs-1))\n",
    "\n",
    "    is_same_cluster = np.ones([n_seqs, n_seqs],np.bool_) \n",
    "    for out in results:\n",
    "        i = n_seqs - out.shape[0] - 1\n",
    "        is_same_cluster[i, i+1:] = out\n",
    "        is_same_cluster[i+1:, i] = out    \n",
    "    meff = 1.0/is_same_cluster.sum(1)\n",
    "    return meff.sum()/(seq_len**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs_ = seqs[:2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.919439094557045\n",
      "481 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "if __name__ == \"__main__\":\n",
    "    meff = get_nf_numpy_mp(seqs_)\n",
    "    print(meff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.919439094557045\n",
      "2.04 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "meff = get_nf_numpy_v2(seqs_)\n",
    "print(meff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benefit of multiprocessing will be determined by hardware. However, it is important to remember that there is an overhead of distributing the work across CPUs. As a result, if the algorithm takes a couple of seconds, multiprocessing might make it even slower.  On the other hand, really big tasks can be speeded up by nearly the factor of number of threads on the machine where overhead becomes negligible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "2020-04-08-spectral-norm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
