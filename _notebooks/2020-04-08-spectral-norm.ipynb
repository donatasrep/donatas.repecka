{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VrE1kciUIxD5"
   },
   "source": [
    "# Hands-on Spectral Normalization\n",
    "> A brief explanation and some code snippets related to spectral normalization  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "skMnA5NeIU4O"
   },
   "source": [
    "Spectral normalization is a widely used technique to stabilize and improve the training of Generative adversarial networks. In nutshell, this normalization technique allows the measurement of meaningful distance between real and generated examples using discriminator. This measure is then used to train both, the discriminator and the generator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OVXv9X4d_dND"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bld73zBz-aO_"
   },
   "source": [
    "In my previous post I have walked through changes proposed in [WGAN paper](https://arxiv.org/abs/1701.07875) skipping an import piece about Lipschitz constraint. In this post I will discuss the most effective (up to this day) technique to satisfy this constraint. \n",
    "\n",
    "As we talked in previous post, work of [the authors of WGAN paper](https://arxiv.org/abs/1701.07875) proposed to use the distance between the outputs of the discriminator as a proxy for distances between distributions. To put it simply, we want to measure how far two distributions are from each other. In order to be able to use the discriminator output as a proxy, we need to bound discriminator to be Lipschitz constrained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pjcWHI7QWErv"
   },
   "source": [
    "## Spectral Normalization\n",
    "\n",
    "Interestingly, [the authors of WGAN paper](https://arxiv.org/abs/1701.07875) turned to the AI scientific community for the best approach. And a year later the paper called [Spectral Normalization for Generative Adversarial Networks](https://arxiv.org/abs/1802.05957) came out with a proposed solution. In comparison to previous attempts, this solution was superior due to its efficiency. So let's dig into it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8-8V_1w2Bvqt"
   },
   "source": [
    "According to the paper, the Lipschitz constraint of the layer can be satisfied by dividing the weights of the layer by the largest singular value of the same weights. A really nice explanation and proof can be found [here](https://christiancosgrove.com/blog/2018/01/04/spectral-normalization-explained.html). Mathematically it looks like this:\n",
    " \n",
    "$$W=W/\\sigma(W)$$\n",
    " \n",
    "where $\\sigma(W)$  is the largest singular value of the weights $W$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w8O-YUYHyjkZ"
   },
   "source": [
    "An important observation by the authors of the paper is that if we do it for all layers, we will have a network that satisfies Lipschitz constraint.\n",
    " \n",
    "Sounds simple enough, let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6JLKK5Jw_HnN"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mofEci9OEcGf"
   },
   "source": [
    "Let's take a convolutional filter of kernel size 3x3, 256 input channels and 512 output channels. Note: we reshaped the kernel so that we could calculate singular values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "VzEk0lnR_HCl",
    "outputId": "7dc00685-fc2d-4e44-f1f0-0031e47f7cb1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2304, 512)"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.random.normal(size = [3,3, 256, 512]).reshape([-1,512])\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-HRyGwQhFES7"
   },
   "source": [
    "Let's use `numpy` library to find singular values. `numpy.linalg.svd` can do that for us. The second output of `numpy.linalg.svd` function returns all singular values, we just need a maximum of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "7PGizDjz_Gks",
    "outputId": "92939744-e099-4d7d-c93d-d311e89e1ade"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 1.02 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "s = np.linalg.svd(W, full_matrices=True)[1].max()\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EFm6f8qSFpca"
   },
   "source": [
    "That is quite simple, however, it takes 1 second to calculate those values. Let's assume that we have 30 layers in the network, that would mean extra 30 seconds for each training step. Well, that is not what we could call efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "39bZ4vip_GPF"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ztj2wiEiZfZB"
   },
   "source": [
    "There is an alternative. `scipy.sparse.linalg.svds` function returns only the number k of the largest singular values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "fDOwxJa4GTqo",
    "outputId": "1b71ec04-634d-4004-bb59-b12d5f5fc528"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 72 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "s = svds(W, k=1)[1]\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Koz7FGYDGhqQ"
   },
   "source": [
    "This is much faster: ~74ms, however for 30 layers we will slow down the training process by 2 seconds per step. Still not ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wlnxyj4SG3oH"
   },
   "source": [
    "Luckly, The authors of the paper proposed an alternative solution to get the largest singular value. This technique is known as power iteration. This looks like this:\n",
    "$$ v = W^\\intercal u / ||W^\\intercal u||_2$$\n",
    " \n",
    "$$ u = W v / ||W v||_2$$\n",
    " \n",
    "$$ W = W / u^\\intercal W v$$\n",
    " \n",
    "Let's see if we can implement it.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3_-V7TLuIYdu"
   },
   "source": [
    "$u$ needs to be sampled from an isotropic distribution at the beginning and it's dimensions should match the dimensions of the number of output channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "KRgztWuX_Fit",
    "outputId": "a4067571-eb6d-4e82-865c-3fd6504c2de5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = np.random.normal(scale=0.2, size=[512])\n",
    "u.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iHl387LhIlPn"
   },
   "source": [
    "First row of the equation\n",
    "\n",
    "*Note: we swapped u and W just for the sake of simplicity when implementing in numpy*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "-ksFiH9Y_FAY",
    "outputId": "810037ad-2bf7-4437-d588-82d87cda55cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2304,)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = u@W.T/np.linalg.norm(u@W.T, 2)\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qBuSCxeoJVzO"
   },
   "source": [
    "Second row of the equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "3XJN0rOzMgqX",
    "outputId": "1023deb9-6932-4bcb-ca25-f45b5f8044c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = v@W / np.linalg.norm(v@W, 2)\n",
    "u.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NdsrL7qxJVuN"
   },
   "source": [
    "Third row of the equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "HuwwOP89Nh_X",
    "outputId": "29f07537-692f-4434-f4a0-ecc0bacd7b76"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53.890708570486424"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma = v@W@u.T\n",
    "sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S-82VLI5MPsW"
   },
   "source": [
    "Let's put everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7nXC8VQ3LWmp"
   },
   "outputs": [],
   "source": [
    "def get_largets_singular_value(u):\n",
    "  v = u@W.T/np.linalg.norm(u@W.T, 2)\n",
    "  u = v@W / np.linalg.norm(v@W, 2)\n",
    "  sigma = v@W@u.T\n",
    "  return sigma, u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "veRM5lBXMA6H",
    "outputId": "c62a9db4-664d-4167-9faf-0d30278d22e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 2.06 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "sigma, _  = get_largets_singular_value(u.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "zifZkGQNMYkw",
    "outputId": "9811f889-fe0d-4284-db46-ce8d79807afe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer from numpy: 70.21844081626173\n",
      "Power iteration outcome 69.4230148155534\n"
     ]
    }
   ],
   "source": [
    "print(\"Answer from numpy:\", svds(W, k=1)[1].squeeze())\n",
    "print(\"Power iteration outcome\", sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gyA0p1rbL_2V"
   },
   "source": [
    "Well, it is fast, ~40x faster, but it is not accurate. Well, that is expected because power iteration only approximates the largest singular value. The more iterations you perform, the more accurate approximation you would get. \n",
    "\n",
    "Let’s see if that is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "id": "S_cA4GaCO5aa",
    "outputId": "5908adf5-c92d-4198-9ac8-311d839c4ef3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power iteration estimate: 60.51 (actual: 70.22)\n",
      "Power iteration estimate: 63.85 (actual: 70.22)\n",
      "Power iteration estimate: 65.64 (actual: 70.22)\n",
      "Power iteration estimate: 66.69 (actual: 70.22)\n",
      "Power iteration estimate: 67.36 (actual: 70.22)\n",
      "Power iteration estimate: 67.82 (actual: 70.22)\n",
      "Power iteration estimate: 68.15 (actual: 70.22)\n",
      "Power iteration estimate: 68.40 (actual: 70.22)\n",
      "Power iteration estimate: 68.59 (actual: 70.22)\n",
      "Power iteration estimate: 68.74 (actual: 70.22)\n",
      "Power iteration estimate: 68.86 (actual: 70.22)\n",
      "Power iteration estimate: 68.97 (actual: 70.22)\n",
      "Power iteration estimate: 69.06 (actual: 70.22)\n",
      "Power iteration estimate: 69.13 (actual: 70.22)\n",
      "Power iteration estimate: 69.19 (actual: 70.22)\n",
      "Power iteration estimate: 69.25 (actual: 70.22)\n",
      "Power iteration estimate: 69.30 (actual: 70.22)\n",
      "Power iteration estimate: 69.35 (actual: 70.22)\n",
      "Power iteration estimate: 69.39 (actual: 70.22)\n",
      "Power iteration estimate: 69.42 (actual: 70.22)\n"
     ]
    }
   ],
   "source": [
    "u_copy = u.copy()\n",
    "actual = svds(W, k=1)[1].squeeze()\n",
    "for i in range(20):\n",
    "  sigma,u_copy = get_largets_singular_value(u_copy)\n",
    "  print(\"Power iteration estimate: {:.2f} (actual: {:.2f})\".format(sigma, actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5xV7Tyb1MXkZ"
   },
   "source": [
    "This is indeed the case. However, doing power iteration 20 times gets us close to `scipy` performance. Fortunately, [Spectral Normalization paper](https://arxiv.org/abs/1802.05957) showed that you can do one or a small number of iterations per training step to get a good estimate throughout the entire training (doing incremental work every step). Hence, even using naive numpy implementation we can get to only extra 60ms (1.7ms*30) for each training iteration (assuming 30 layer network).\n",
    " \n",
    "That does not sound too bad.\n",
    " \n",
    "*Note: keep in mind that these performance numbers are relative, they will depend on hardware.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WS-fl77Cc5sk"
   },
   "source": [
    "We can try to port this to tensorflow and measure its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "oG7SGdw0L18_",
    "outputId": "11acb7dc-0e58-4f50-a8a3-1e28775531a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QLZpsHL0PYJC"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_largets_singular_value_tf(u, w):\n",
    "  _v = tf.matmul(u, w, transpose_b=True)\n",
    "  _v = tf.math.l2_normalize(_v)\n",
    "  _u_m = tf.matmul(_v, w)\n",
    "  _u = tf.math.l2_normalize(_u_m)\n",
    "  sigma = tf.matmul(_u_m, _u, transpose_b=True)\n",
    "  return sigma, _u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aNROYMuiQM-5"
   },
   "outputs": [],
   "source": [
    "u_tf = tf.Variable(tf.random.normal(stddev=0.2, shape=[1,512]), trainable=False)\n",
    "w_tf = tf.Variable(tf.random.normal(shape = [3*3*256, 512]), trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "EJh_I6faP0Ep",
    "outputId": "30526b78-c70d-48ca-be0e-3da5f7cc1fa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 65.11 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1000 loops, best of 3: 509 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "sigma_tf, _ = get_largets_singular_value_tf(u_tf, w_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "vwaIZdFhQnie",
    "outputId": "49f65e7a-0eca-443a-ea78-b82189887c58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[69.43079]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "  sigma_tf, u_tf = get_largets_singular_value_tf(u_tf, w_tf)\n",
    "print(sigma_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BLEqb0skROZn"
   },
   "source": [
    "That is even better. 509µs with tensorflow implementation and executed on GPU (which usually is the case when you are training GANs). So if we assume 30 layers of similar size and one power iteration per training step, we only need ~15ms extra for the step. That is the reason why this technique is widely and successfully adopted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s3N07b6OetiL"
   },
   "source": [
    "# In practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aX4DqkJwfFiq"
   },
   "source": [
    "Since we have all the pieces, we can create a layer wrapper to wrap any layer and perform power iteration on each feed forward pass.\n",
    "I have extended the code of [here](https://medium.com/@FloydHsiu0618/spectral-normalization-implementation-of-tensorflow-2-0-keras-api-d9060d26de77)\n",
    "I have incorporated suggestions from the comments:\n",
    "- Turned off power iterations during the inherence. This makes sure that weights are not being changed doing inherence.\n",
    "- Used assigned operation instead of `=`. This speeds up the algorithm quite significantly (roughly by 35%). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bU9RpIvGhiRB"
   },
   "source": [
    "Additionally I have also added a hack to support mixed precision as well as the logic to support embedding layers. The final solution looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eYzUmolgeyYf"
   },
   "outputs": [],
   "source": [
    "class SpectralNormalizationV2(tf.keras.layers.Wrapper):\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "       layer: tensorflow keras layers (with kernel or embedding attribute)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer, eps=1e-12, **kwargs):\n",
    "        super(SpectralNormalizationV2, self).__init__(layer, name=layer.name + \"_sn\", **kwargs)\n",
    "        self.eps = eps\n",
    "        self.is_embedding = isinstance(self.layer, tf.keras.layers.Embedding)\n",
    "\n",
    "    def get_kernel_variable(self, attr='kernel'):\n",
    "        if not hasattr(self.layer, attr):\n",
    "            raise ValueError('`SpectralNormalization` must wrap a layer that contains a `{}` for weights'.format(attr))\n",
    "        return getattr(self.layer, attr)\n",
    "\n",
    "\n",
    "    def build(self, input_shape=None):\n",
    "        if not self.built:\n",
    "            super(SpectralNormalizationV2, self).build(input_shape)\n",
    "            if self.is_embedding:\n",
    "                self.w = self.get_kernel_variable(\"embeddings\")\n",
    "            else:\n",
    "                self.w = self.get_kernel_variable()\n",
    "\n",
    "            self.autocast = hasattr(self.w, \"_variable\")\n",
    "            self.last_dim = self.w.shape[-1]\n",
    "            self.u = self.add_weight(shape=[1, self.last_dim],\n",
    "                                     initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),\n",
    "                                     name='sn_u',\n",
    "                                     trainable=False,\n",
    "                                     experimental_autocast=False)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=True):\n",
    "        # Recompute weights for each training forward pass\n",
    "        if training:\n",
    "            self._compute_weights()\n",
    "        output = self.layer(inputs, training=training)\n",
    "        return output\n",
    "\n",
    "    def _compute_weights(self):\n",
    "        \"\"\"Generate normalized weights.\n",
    "        This method will update the value of self.layer.kernel with the\n",
    "        normalized value, so that the layer is ready for call().\n",
    "        \"\"\"\n",
    "        if self.autocast:\n",
    "            w = self.w._variable\n",
    "        else:\n",
    "            w =self.w\n",
    "        w_reshaped = tf.reshape(w, [-1, self.last_dim])\n",
    "        _v = tf.matmul(self.u, w_reshaped, transpose_b=True)\n",
    "        _v = tf.math.l2_normalize(_v, epsilon=self.eps)\n",
    "        _u_m = tf.matmul(_v, w_reshaped)\n",
    "        _u = tf.math.l2_normalize(_u_m, epsilon=self.eps)\n",
    "        sigma = tf.matmul(_u_m, _u, transpose_b=True)\n",
    "\n",
    "        self.u.assign(_u)\n",
    "        self.w.assign(w / sigma)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return self.layer.compute_output_shape(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wAI2gb0u49-k"
   },
   "source": [
    "And the way you use is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MZxAzzu048ap"
   },
   "outputs": [],
   "source": [
    "dense_sn = SpectralNormalizationV2(tf.keras.layers.Dense(units=100))\n",
    "\n",
    "conv_sn = SpectralNormalizationV2(tf.keras.layers.Conv2D(filters=256, kernel_size=3))\n",
    "\n",
    "emb_sn = SpectralNormalizationV2(tf.keras.layers.Embedding(input_dim=20, output_dim=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "shSm0dbd5r8r"
   },
   "source": [
    "I have also checked the performance in comparison to [Pytorch implementation](https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/spectral_norm.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CAtxNdwS7ut1"
   },
   "source": [
    "Setup:\n",
    "- Batch size: 64\n",
    "- Kernel of size :[16, 16, 256, 512]\n",
    "- Steps: 1000\n",
    "- One power iteration per step\n",
    "- Hardware: NVIDIA V100\n",
    "\n",
    "\n",
    "Results are: \n",
    "- Elapse time of Tensorflow SpectralNormalizationV2:  1.1081128120422363s\n",
    "- Elapse time of Pytorch official SpectralNorm implementation:  1.0783729553222656s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Clxgtr28Mh8"
   },
   "source": [
    "Pytorch seems to be a little bit faster. I am not entirely sure whether it has to do with implementations of spectral normalization or just to the differences between Pytorch and Tensorflow. Regardless, those extra 30ms every 1000 steps should not be a game changer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "981KunF_99Ow"
   },
   "source": [
    "# Summary\n",
    "Spectral normalization is quite widely used in various implementations of GANs. One of the most famous applications is [Biggan](https://arxiv.org/abs/1809.11096). All images below are generated by BigGAN architecture that uses Spectral normalization. This should highlight the importance of this advancement. \n",
    "\n",
    "Throughout this post we focused on the implementation of power iteration proposed in [Spectral Normalization for Generative Adversarial Networks paper](https://arxiv.org/abs/1802.05957). We have compared various methods to calculate the largest singular value and observed that the proposed power iteration method is the fastest and after multiple iterations it achieves reasonable accuracy.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xT_cry5MSila"
   },
   "source": [
    "![alt text](https://miro.medium.com/max/2468/1*Yw2KxjmIkj8yqS-ykLCQCQ.png)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "2020-04-08-spectral-norm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
