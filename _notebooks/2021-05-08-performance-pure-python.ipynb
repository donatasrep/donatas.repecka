{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VrE1kciUIxD5"
   },
   "source": [
    "# Performance Analysis -  Pure Python\n",
    "> Number of effective sequences implemented in Pure Python\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- author: Donatas Repečka\n",
    "- categories: [performance]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OVXv9X4d_dND"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "skMnA5NeIU4O"
   },
   "source": [
    "In [the previous post](https://donatasrep.github.io/donatas.repecka/performance/2021/04/27/Performance-comparison.html) I have compared various languages and libraries in terms of their speed. This notebook contains the code used in the comparison as well as some details about the choices made to improve the performance of pure python implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/donatasrep/donatas.repecka/blob/master/data/picked_msa.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install numpy\n",
    "# ! pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will cheat here and use pandas to help me to read the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    fasta_df = pd.read_csv(path, sep=\"\\n\", lineterminator=\">\", index_col=False, names=['id', 'seq'])\n",
    "    return fasta_df.seq.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = get_data('../data/picked_msa.fasta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python naive implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to remind the pseudo code looks like this:\n",
    "\n",
    "```\n",
    "for seq1 in seqs:\n",
    "  for seq2 in seqs:\n",
    "    if count_mathes(seq1, seq2) > threshold:\n",
    "      weight +=1\n",
    "  meff += 1/weight\n",
    " \n",
    "meff = meff/(len(seq1)^0.5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It translates into python relatively easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nf_python(seqs, threshold=0.8):\n",
    "    n_seqs, seq_len = len(seqs), len(seqs[0])\n",
    "    meff = 0\n",
    "    for seq1 in seqs:\n",
    "        c  = 0\n",
    "        for seq2 in seqs:\n",
    "            identity = 0\n",
    "            for p in range(seq_len):                \n",
    "                identity = identity + int(seq1[p] == seq2[p])\n",
    "            is_more = int((identity/seq_len) > threshold)\n",
    "            c = c + is_more\n",
    "        meff = meff + 1/c\n",
    "    return meff/(seq_len**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is `O((n^2)*l)` complexity (where `l` is length in the sequence) as we need to go through the n sequences for every n sequences (pairwise operation) and compare each element in the sequence. This is not ideal and the scale of such an algorithm is poor. If interested you can read more about this [here](https://www.geeksforgeeks.org/analysis-algorithms-big-o-analysis/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.86 s ± 46.6 ms per loop (mean ± std. dev. of 3 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 3 -r 3\n",
    "get_nf_python(seqs[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18006706787628668"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nf_python(seqs[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, the algorithm can be improved by exploiting the fact that number of matches are the same between `seq1`, `seq2` and sequences `seq2`, `seq1` (the pairwise matrix is symmetric)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nf_python_v2(seqs, threshold=0.8):\n",
    "    n_seqs, seq_len = len(seqs), len(seqs[0])\n",
    "    is_same_cluster = [[ 1 for _ in range(n_seqs)] for _ in range(n_seqs)]\n",
    "    meff = 0\n",
    "    for i in range(n_seqs):\n",
    "        for j in range(i+1,n_seqs):\n",
    "            identity = 0\n",
    "            for p in range(seq_len):                \n",
    "                identity = identity + int(seqs[j][p] == seqs[i][p])\n",
    "            is_more = int((identity/seq_len) > threshold)\n",
    "            is_same_cluster[i][j]=is_more\n",
    "            is_same_cluster[j][i]=is_more\n",
    "        c = sum(is_same_cluster[i])\n",
    "        meff = meff + 1/c\n",
    "    return meff/(seq_len**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11 s ± 8.22 ms per loop (mean ± std. dev. of 3 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 3 -r 3\n",
    "get_nf_python_v2(seqs[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18006706787628668"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nf_python_v2(seqs[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although, technically, this version is still `O((n^2)*l)`, but in practise the speed improvement will be substantial (in this case, improvement is roughly 40%). Also, it is possible to break the last inner loop earlier, however, I did not see any performance gain (cost `if` statement offset the gain from early stopping). \n",
    "\n",
    "Note that I used only 1% of the data. Running on these algorithms on full data set would take too long and hence not recommended.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "2020-04-08-spectral-norm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
